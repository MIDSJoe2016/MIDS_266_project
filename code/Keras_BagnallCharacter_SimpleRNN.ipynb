{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import random as rn\n",
    "rn.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Keras_BagnallCharacter_SimpleRNN.ipynb to script\n",
      "[NbConvertApp] Writing 14441 bytes to Keras_BagnallCharacter_SimpleRNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Keras_BagnallCharacter_SimpleRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 speeches for 2 presidents.\n",
      "Loaded 1747 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "    \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "    \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "    \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "    \"Ford\": \"Gerald R. Ford\",\n",
    "    \"Truman\": \"Harry S. Truman\",\n",
    "    \"Hoover\": \"Herbert Hoover\",\n",
    "    \"Carter\": \"Jimmy Carter\",\n",
    "    \"Kennedy\": \"John F. Kennedy\",\n",
    "    \"Johnson\": \"Lyndon B. Johnson\",\n",
    "    \"Nixon\": \"Richard Nixon\",\n",
    "    \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    "\n",
    "print \"Replacements complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you. Thank you, everybody. I am thrilled to be here in the great state of Kentucky and the beautiful city of Louisville And this place is packed. There are a lot of people outside that arent getting in, but thats all right. We love them, too, right? We love them, too. \n",
      "Were in the heartland of America, and there is no place I would rather be than here with you, tonight. Our first Republican President, Abraham Lincoln, was born right here in Kentucky. Thats not bad. The legendary pioneer, Daniel Boone, helped settle the Kentucky frontier. And the great 7th century American statesman, Henry Clay, represented Kentucky in the United States Congress. Henry Clay believed in what he called the American system, and proposed tariffs to protect\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a scrubbed text excerpt\n",
    "print loaded_text[10][:750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4869126\n",
      "1  : Donald J. Trump      \t555750\n",
      "2  : Dwight D. Eisenhower \t3062523\n",
      "3  : Franklin D. Roosevelt \t1741086\n",
      "4  : George Bush          \t1886633\n",
      "5  : George W. Bush       \t1772167\n",
      "6  : Gerald R. Ford       \t706711\n",
      "7  : Harry S. Truman      \t1988667\n",
      "8  : Herbert Hoover       \t778333\n",
      "9  : Jimmy Carter         \t1283640\n",
      "10 : John F. Kennedy      \t1337590\n",
      "11 : Lyndon B. Johnson    \t2318863\n",
      "12 : Richard Nixon        \t969424\n",
      "13 : Ronald Reagan        \t982788\n",
      "14 : William J. Clinton   \t1765554\n",
      "\n",
      "Minimum number of characters per president?\n",
      "555750\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4692463, ' '), (2550095, 'e'), (2117905, 't'), (1615001, 'o'), (1598810, 'a'), (1465361, 'n'), (1394749, 'i'), (1183917, 's'), (1147156, 'r'), (1059389, 'h'), (749147, 'l'), (699363, 'd'), (544092, 'u'), (529907, 'c'), (475112, 'm'), (415048, 'w'), (390669, 'g'), (385818, 'f'), (379109, 'p'), (371935, 'y'), (279440, 'b'), (262578, ','), (262036, '.'), (239101, 'v'), (175280, 'I'), (150340, 'k'), (95803, \"'\"), (74330, 'A'), (58657, 'T'), (58270, 'S'), (44149, '-'), (40497, '7'), (40258, 'W'), (35453, 'C'), (33702, 'N'), (32672, 'x'), (27565, 'M'), (25721, 'P'), (25335, 'j'), (25299, 'B'), (23864, 'E'), (21606, 'q'), (20219, 'R'), (19871, 'O'), (19463, 'G'), (17125, 'H'), (16203, 'U'), (15821, 'D'), (13768, 'F'), (12806, 'L'), (11786, 'z'), (10651, 'Y'), (10586, '\"'), (9196, ';'), (8111, '?'), (8043, 'J'), (6308, ':'), (5273, 'V'), (4481, 'K'), (4083, '\\n'), (3981, '$'), (1853, '/'), (1348, 'Q'), (446, 'Z'), (421, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 1040747 1040747\n",
      "\n",
      "Total characters: 26018675\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 50\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 9, 5, 3, 7, 4, 6, 1, 10, 2, 1, 10, 5, 8, 1, 8, 10, 4, 16, 6, 1, 3, 10, 9, 4], [13, 17, 10, 4, 13, 3, 1, 3, 10, 7, 8, 1, 3, 9, 5, 6, 8, 7, 3, 7, 4, 6, 23, 60, 49], [4, 9, 3, 20, 31, 18, 4, 13, 9, 1, 28, 15, 2, 9, 7, 14, 5, 6, 8, 1, 10, 5, 24, 2, 1], [6, 4, 16, 1, 3, 5, 26, 2, 6, 1, 3, 10, 2, 1, 38, 9, 2, 8, 7, 12, 2, 6, 3, 7, 5], [11, 1, 4, 5, 3, 10, 23, 1, 29, 10, 2, 1, 16, 4, 9, 12, 8, 1, 10, 5, 24, 2, 1, 21, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few split text excerpts\n",
    "print split_text[10:15]\n",
    "print split_labels[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples; shuffle if requested\n",
    "#\n",
    "import sklearn.utils\n",
    "\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8, shuffle_p=True):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "\n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "\n",
    "    if shuffle_p:\n",
    "        test_text, test_labels = sklearn.utils.shuffle(test_text, test_labels)\n",
    "        train_text, train_labels = sklearn.utils.shuffle(train_text, train_labels)\n",
    "\n",
    "    return train_text, train_labels, test_text, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splits:\n",
      " Test =  832592 \n",
      " Train =  208155\n",
      "\n",
      "Class weights:\n",
      "{0: 0.35623785930052454, 1: 3.1211276053381316, 2: 0.5663891156462585, 3: 0.9962690406959351, 4: 0.9194019302546435, 5: 0.9788060473536949, 6: 2.454503110167743, 7: 0.8722442223479372, 8: 2.2286249631949464, 9: 1.3513032752296557, 10: 1.2968116754668786, 11: 0.7480308523015691, 12: 1.7893660004298302, 13: 1.7650131433901466, 14: 0.9824616056309775}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.8, shuffle_p=False)\n",
    "print \"Sample splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"\\nClass weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 9, 5, 3, 7, 4, 6, 1, 10, 2, 1, 10, 5, 8, 1, 8, 10, 4, 16, 6, 1, 3, 10, 9, 4], [13, 17, 10, 4, 13, 3, 1, 3, 10, 7, 8, 1, 3, 9, 5, 6, 8, 7, 3, 7, 4, 6, 23, 60, 49], [4, 9, 3, 20, 31, 18, 4, 13, 9, 1, 28, 15, 2, 9, 7, 14, 5, 6, 8, 1, 10, 5, 24, 2, 1], [6, 4, 16, 1, 3, 5, 26, 2, 6, 1, 3, 10, 2, 1, 38, 9, 2, 8, 7, 12, 2, 6, 3, 7, 5], [11, 1, 4, 5, 3, 10, 23, 1, 29, 10, 2, 1, 16, 4, 9, 12, 8, 1, 10, 5, 24, 2, 1, 21, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few of the split text excerpts; \n",
    "# likely the same classes based on the split point\n",
    "print train_X[10:15]\n",
    "print train_y[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (832592, 25)\n",
      "...to  (20814800, 68)\n",
      "...and reshaping to  (832592, 25, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (208155, 25)\n",
      "...to  (5203875, 68)\n",
      "...and reshaping to  (208155, 25, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Have a again look at a few of the split and encoded text excerpts; \n",
    "# both arrays should be one-hot encoded.\n",
    "print train_X[10:11]\n",
    "print train_y[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom activation from Bagnall 2015\n",
    "#  we were never able to get this to work; either nan'ed or never converged\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def ReSQRT(x):\n",
    "    cond = tf.less_equal(x, 0.0)\n",
    "    return tf.where(cond, x * 0.0, tf.sqrt(x+1)-1)\n",
    "\n",
    "get_custom_objects().update({'ReSQRT': ReSQRT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bagnall proposes that the following possible values contribute to the success of the model:\n",
    "\n",
    "| meta-parameter                  \t| typical value                      \t|\n",
    "|---------------------------------\t|------------------------------------\t|\n",
    "| initial adagrad learning scale  \t| 0.1, 0.14, 0.2, 0.3                \t|\n",
    "| initial leakage between classes \t| 1/4N to 5/N                        \t|\n",
    "| leakage decay (per sub-epoch)   \t| 0.67 to 0.9                        \t|\n",
    "| hidden neurons                  \t| 79, 99, 119, 139                   \t|\n",
    "| presynaptic noise σ             \t| 0, 0.1, 0.2, 0.3, 0.5              \t|\n",
    "| sub-epochs                      \t| 6 to 36                            \t|\n",
    "| text direction                  \t| forward or backward                \t|\n",
    "| text handling                   \t| sequential, concatenated, balanced \t|\n",
    "| initialisation                  \t| gaussian, zero                     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 25, 68)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 200)               33800     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 15)                3015      \n",
      "=================================================================\n",
      "Total params: 36,815\n",
      "Trainable params: 36,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "143650/832592 [====>.........................] - ETA: 432s - loss: 2.7122 - categorical_accuracy: 0.0650"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-a602e84898c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           verbose=1)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keras_BagnallCharacter_SimpleRNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define operating vars\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "# define optimizer\n",
    "optimizer = Adagrad(lr=0.001)\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='categorical_accuracy', factor=0.5,\n",
    "              patience=1, verbose=1)\n",
    "csv_logger = CSVLogger('Keras_BagnallCharacter_SimpleRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=100,activation=\"tanh\"))(main_input)\n",
    "drop = Dropout(0.2)(rnn)\n",
    "main_output = Dense(len(labels),activation='softmax',kernel_initializer='random_uniform')(drop)\n",
    "model = Model(inputs=[main_input], outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "plot_model(model, to_file='Keras_BagnallCharacter_SimpleRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# train\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=1)\n",
    "\n",
    "model.save('Keras_BagnallCharacter_SimpleRNN.h5')  \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_BagnallCharacter_SimpleRNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        print(np.sum(cm,axis=0))\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(np.round(cnf_matrix,2), classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
