{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Keras_Character_MultiHeadRNN.ipynb to script\n",
      "[NbConvertApp] Writing 18411 bytes to Keras_Character_MultiHeadRNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Keras_Character_MultiHeadRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 speeches for 14 presidents.\n",
      "Loaded 1764 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "    \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "    \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "    \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "    \"Ford\": \"Gerald R. Ford\",\n",
    "    \"Truman\": \"Harry S. Truman\",\n",
    "    \"Hoover\": \"Herbert Hoover\",\n",
    "    \"Carter\": \"Jimmy Carter\",\n",
    "    \"Kennedy\": \"John F. Kennedy\",\n",
    "    \"Johnson\": \"Lyndon B. Johnson\",\n",
    "    \"Nixon\": \"Richard Nixon\",\n",
    "    \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character clean-up complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    "     \n",
    "    # REPLACE WORD IN ALL CAPS with <space>; headers\n",
    "    loaded_text[x] = re.sub('[A-Z]{2,}','', loaded_text[x])\n",
    "\n",
    "print \"Character clean-up complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4862045\n",
      "1  : Donald J. Trump      \t554978\n",
      "2  : Dwight D. Eisenhower \t3084215\n",
      "3  : Franklin D. Roosevelt \t1743457\n",
      "4  : George Bush          \t1896536\n",
      "5  : George W. Bush       \t1791014\n",
      "6  : Gerald R. Ford       \t687272\n",
      "7  : Harry S. Truman      \t2001118\n",
      "8  : Herbert Hoover       \t786184\n",
      "9  : Jimmy Carter         \t1272152\n",
      "10 : John F. Kennedy      \t1344012\n",
      "11 : Lyndon B. Johnson    \t2288606\n",
      "12 : Richard Nixon        \t972275\n",
      "13 : Ronald Reagan        \t1010291\n",
      "14 : William J. Clinton   \t1784166\n",
      "\n",
      "Minimum number of characters per president?\n",
      "554978\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4725447, ' '), (2569974, 'e'), (2131220, 't'), (1627649, 'o'), (1609954, 'a'), (1476541, 'n'), (1405067, 'i'), (1193120, 's'), (1157275, 'r'), (1066798, 'h'), (755187, 'l'), (705084, 'd'), (548669, 'u'), (534278, 'c'), (478832, 'm'), (418343, 'w'), (393337, 'g'), (389784, 'f'), (382020, 'p'), (374441, 'y'), (281512, 'b'), (264600, ','), (263713, '.'), (240856, 'v'), (162701, 'I'), (151063, 'k'), (95930, \"'\"), (62618, 'A'), (49111, 'S'), (47587, 'T'), (44390, '-'), (40548, '7'), (39451, 'W'), (32811, 'x'), (29785, 'C'), (25503, 'j'), (23659, 'B'), (23374, 'M'), (22167, 'N'), (21726, 'q'), (21723, 'P'), (16943, 'G'), (14374, 'H'), (13620, 'U'), (12142, 'D'), (11930, 'z'), (11837, 'R'), (10677, 'F'), (10672, '\"'), (10127, 'E'), (9435, 'O'), (9370, 'Y'), (9360, ';'), (8141, '?'), (7920, 'J'), (7822, 'L'), (6390, ':'), (4737, '\\n'), (3981, '$'), (3943, 'K'), (3705, 'V'), (1853, '/'), (1066, 'Q'), (338, 'Z'), (80, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 1043126 1043126\n",
      "\n",
      "Total characters: 26078150\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size, window=False):\n",
    "    output_list = []\n",
    "    if (window):\n",
    "        for idx in range(0, len(_list)-_split_size):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    else:\n",
    "        for idx in range(0, len(_list), _split_size):\n",
    "            if (idx + _split_size) <= len(_list):\n",
    "                output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 25\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples\n",
    "#\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "        \n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "        \n",
    "    return train_text,train_labels,test_text,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      " Test =  834494 \n",
      " Train =  208632\n",
      "Class weights:\n",
      "{0: 0.35757490058960645, 1: 3.1326613735758393, 2: 0.5636911396167278, 3: 0.9971846806476669, 4: 0.916704016170138, 5: 0.9707030522985297, 6: 2.5296895840911846, 7: 0.8687894640951563, 8: 2.2114295557233903, 9: 1.3666339130719596, 10: 1.2935484871031746, 11: 0.7596495300516601, 12: 1.7881503385617554, 13: 1.7208900437185515, 14: 0.9744435881267661}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.8)\n",
    "print \"Splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"Class weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (834494, 25)\n",
      "...to  (20862350, 68)\n",
      "...and reshaping to  (834494, 25, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (208632, 25)\n",
      "...to  (5215800, 68)\n",
      "...and reshaping to  (208632, 25, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = np.array(to_categorical(train_y))\n",
    "test_y = np.array(to_categorical(test_y))\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom activation from Bagnall 2015\n",
    "#  does not appear to perform as well as a ReLU\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def ReSQRT(x):\n",
    "    cond = tf.less_equal(x, 0.0)\n",
    "    result = tf.where(cond, x * 0.0, tf.sqrt(x+1)-1)\n",
    "    return result\n",
    "\n",
    "get_custom_objects().update({'ReSQRT': ReSQRT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bagnall proposes that the following possible values contribute to the success of the model:\n",
    "\n",
    "| meta-parameter                  \t| typical value                      \t|\n",
    "|---------------------------------\t|------------------------------------\t|\n",
    "| initial adagrad learning scale  \t| 0.1, 0.14, 0.2, 0.3                \t|\n",
    "| initial leakage between classes \t| 1/4N to 5/N                        \t|\n",
    "| leakage decay (per sub-epoch)   \t| 0.67 to 0.9                        \t|\n",
    "| hidden neurons                  \t| 79, 99, 119, 139                   \t|\n",
    "| presynaptic noise σ             \t| 0, 0.1, 0.2, 0.3, 0.5              \t|\n",
    "| sub-epochs                      \t| 6 to 36                            \t|\n",
    "| text direction                  \t| forward or backward                \t|\n",
    "| text handling                   \t| sequential, concatenated, balanced \t|\n",
    "| initialisation                  \t| gaussian, zero                     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 25, 68)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 68)            18632       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 15)            1035        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 15)            0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "                                                                   dense_3[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "                                                                   dense_5[0][0]                    \n",
      "                                                                   dense_6[0][0]                    \n",
      "                                                                   dense_7[0][0]                    \n",
      "                                                                   dense_8[0][0]                    \n",
      "                                                                   dense_9[0][0]                    \n",
      "                                                                   dense_10[0][0]                   \n",
      "                                                                   dense_11[0][0]                   \n",
      "                                                                   dense_12[0][0]                   \n",
      "                                                                   dense_13[0][0]                   \n",
      "                                                                   dense_14[0][0]                   \n",
      "                                                                   dense_15[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 34,157\n",
      "Trainable params: 34,157\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Multi-head RNN\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional\n",
    "from keras.layers.merge import Maximum, Add, Concatenate\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.merge import Average, Maximum\n",
    "from keras.optimizers import Adamax, RMSProp\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# set parameters; determined by optimization @ end\n",
    "init_modes = 'random_normal'\n",
    "batch_size = unique_chars\n",
    "units = unique_chars\n",
    "dropout = 0.2\n",
    "activation = 'relu'\n",
    "merge_mode = 'sum' #concat'\n",
    "shuffle = False #True\n",
    "\n",
    "optimizer = RMSProp(lr=0.01) #'Adamax'\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                              factor=0.1,\n",
    "                              patience=1, \n",
    "                              verbose=1)\n",
    "csv_logger = CSVLogger('Keras_Character_MultiHeadRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=units,\n",
    "                              activation=activation,\n",
    "                              recurrent_dropout=dropout,\n",
    "                              kernel_initializer=init_modes),\n",
    "                    merge_mode=merge_mode)(main_input)\n",
    "\n",
    "soft_out = []\n",
    "for idx in range(0,len(labels)):\n",
    "    soft_out.append(Dense(len(labels),\n",
    "                          activation='softmax', \n",
    "                          kernel_initializer=init_modes)(rnn))\n",
    "final_out = Add()(soft_out)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs = final_out) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "plot_model(model, to_file='Keras_Character_MultiHeadRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=2)\n",
    "\n",
    "model.save('Keras_Character_MultiHeadRNN.h5')\n",
    "print (\"Model saved.\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## MODEL EVALUATION\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_Character_MultiHeadRNN.h5')\n",
    "print \"Model loaded.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "batch_size = 50# 100\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot normalized confusion matrix\n",
    "cnf_matrix_pct = cnf_matrix *1.0\n",
    "cnf_matrix_pct = np.around(np.array([row*100.0/sum(row) for row in cnf_matrix_pct]), 2)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix_pct, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix on accuracy percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binwidth = .05\n",
    "pred_outs = pred_y/len(labels)\n",
    "plt.hist(pred_outs.max(axis=1),bins=np.arange(0.0, 1.0, 0.05))\n",
    "plt.title('Frequency of predicted max probability per sequence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_idx = 20000\n",
    "print \"Predicted President: \", np.argmax(labels[pred_y[sample_idx]])\n",
    "print \"Actual President: \", np.argmax(labels[test_y[sample_idx]])\n",
    "print split_text[sample_idx-1:sample_idx+1]\n",
    "\n",
    "# print one sequence on either side of confused\n",
    "sample = split_text[sample_idx-1:sample_idx+1]\n",
    "sample = sum(sample, [])\n",
    "sample_txt = \"\"\n",
    "tokenizer_rev = {v: k for k, v in tokenizer.word_index.items()}\n",
    "for char in sample:\n",
    "    sample_txt += tokenizer_rev[char]\n",
    "print sample_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## MODEL OPTIMIZATION\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional\n",
    "from keras.layers.merge import Maximum, Add, Concatenate\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.merge import Average, Maximum\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# define operating vars\n",
    "# activation = \"relu\" #\"ReSQRT\" \n",
    "# units = 150# 50\n",
    "# dropout = 0.0 #0.7646166765488501\n",
    "# batch_size = 50# 100\n",
    "# epochs = 100\n",
    "# optimizer='adamax'#'rmsprop'\n",
    "# shuffle=True #False\n",
    "\n",
    "def create_model(optimizer='rmsprop', learn_rate=0.01,\n",
    "                 init_mode1='glorot_uniform', init_mode2='glorot_uniform', \n",
    "                 merge_mode='ave', activation='relu', \n",
    "                 dropout_rate=0.0, neuron_count=50):\n",
    "\n",
    "    # assemble & compile model\n",
    "    input = Input(shape=(max_seq_len,unique_chars,))\n",
    "    rnn = Bidirectional(SimpleRNN(units=neuron_count,\n",
    "                                  activation=activation,\n",
    "                                  recurrent_dropout=dropout_rate,\n",
    "                                  kernel_initializer=init_mode1),\n",
    "                        merge_mode=merge_mode)(input)\n",
    "    \n",
    "    soft_out = []\n",
    "    for idx in range(0,len(labels)):\n",
    "        soft_out.append(Dense(len(labels),\n",
    "                              activation='softmax', \n",
    "                              kernel_initializer=init_mode2)(rnn))\n",
    "    final_out = Add()(soft_out)\n",
    "\n",
    "    model = Model(inputs=[input], outputs = final_out) \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "epoch = [2]\n",
    "batch_sizes = [25,50,75]\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adamax']\n",
    "init_modes1 = ['uniform', 'normal', 'zero']\n",
    "merge_modes = ['sum', 'mul', 'concat', 'ave']\n",
    "activations = ['relu','tanh']\n",
    "dropout_rates = [0.0,0.5,0.9]\n",
    "neuron_counts = [25,50,75]\n",
    "learn_rates = [0.01,0.1,0.2]\n",
    "param_grid = dict(batch_size=batch_sizes,\n",
    "                  epochs=epoch,\n",
    "                  optimizer=optimizers,\n",
    "                  learn_rate=learn_rates,\n",
    "                  init_mode1=init_modes1,\n",
    "                  init_mode2=init_modes2,\n",
    "                  merge_mode=merge_modes,\n",
    "                  activation=activations,\n",
    "                  dropout_rate=dropout_rates,\n",
    "                  neuron_count=neuron_counts)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=2, cv=2)\n",
    "grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "# summarize results\n",
    "# from http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
