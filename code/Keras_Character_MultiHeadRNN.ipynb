{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern u'Keras_BagnallCharacter_MultiHeadRNN.ipynb' matched no files\r\n",
      "This application is used to convert notebook files (*.ipynb) to various other\r\n",
      "formats.\r\n",
      "\r\n",
      "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\r\n",
      "\r\n",
      "Options\r\n",
      "-------\r\n",
      "\r\n",
      "Arguments that take values are actually convenience aliases to full\r\n",
      "Configurables, whose aliases are listed on the help line. For more information\r\n",
      "on full configurables, see '--help-all'.\r\n",
      "\r\n",
      "--execute\r\n",
      "    Execute the notebook prior to export.\r\n",
      "--allow-errors\r\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\r\n",
      "--stdout\r\n",
      "    Write notebook output to stdout instead of files.\r\n",
      "--stdin\r\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\r\n",
      "--inplace\r\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \r\n",
      "    relevant when converting to notebook format)\r\n",
      "-y\r\n",
      "    Answer yes to any questions instead of prompting.\r\n",
      "--debug\r\n",
      "    set log level to logging.DEBUG (maximize logging output)\r\n",
      "--generate-config\r\n",
      "    generate default config file\r\n",
      "--nbformat=<Enum> (NotebookExporter.nbformat_version)\r\n",
      "    Default: 4\r\n",
      "    Choices: [1, 2, 3, 4]\r\n",
      "    The nbformat version to write. Use this to downgrade notebooks.\r\n",
      "--output-dir=<Unicode> (FilesWriter.build_directory)\r\n",
      "    Default: ''\r\n",
      "    Directory to write output to.  Leave blank to output to the current\r\n",
      "    directory\r\n",
      "--writer=<DottedObjectName> (NbConvertApp.writer_class)\r\n",
      "    Default: 'FilesWriter'\r\n",
      "    Writer class used to write the  results of the conversion\r\n",
      "--log-level=<Enum> (Application.log_level)\r\n",
      "    Default: 30\r\n",
      "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\r\n",
      "    Set the log level by value or name.\r\n",
      "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\r\n",
      "    Default: u''\r\n",
      "    The URL prefix for reveal.js. This can be a a relative URL for a local copy\r\n",
      "    of reveal.js, or point to a CDN.\r\n",
      "    For speaker notes to work, a local reveal.js prefix must be used.\r\n",
      "--to=<Unicode> (NbConvertApp.export_format)\r\n",
      "    Default: 'html'\r\n",
      "    The export format to be used, either one of the built-in formats, or a\r\n",
      "    dotted object name that represents the import path for an `Exporter` class\r\n",
      "--template=<Unicode> (TemplateExporter.template_file)\r\n",
      "    Default: u''\r\n",
      "    Name of the template file to use\r\n",
      "--output=<Unicode> (NbConvertApp.output_base)\r\n",
      "    Default: ''\r\n",
      "    overwrite base name use for output files. can only be used when converting\r\n",
      "    one notebook at a time.\r\n",
      "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\r\n",
      "    Default: u''\r\n",
      "    PostProcessor class used to write the results of the conversion\r\n",
      "--config=<Unicode> (JupyterApp.config_file)\r\n",
      "    Default: u''\r\n",
      "    Full path of a config file.\r\n",
      "\r\n",
      "To see all available configurables, use `--help-all`\r\n",
      "\r\n",
      "Examples\r\n",
      "--------\r\n",
      "\r\n",
      "    The simplest way to use nbconvert is\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb\r\n",
      "    \r\n",
      "    which will convert mynotebook.ipynb to the default format (probably HTML).\r\n",
      "    \r\n",
      "    You can specify the export format with `--to`.\r\n",
      "    Options include ['custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\r\n",
      "    \r\n",
      "    > jupyter nbconvert --to latex mynotebook.ipynb\r\n",
      "    \r\n",
      "    Both HTML and LaTeX support multiple output templates. LaTeX includes\r\n",
      "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\r\n",
      "    can specify the flavor of the format used.\r\n",
      "    \r\n",
      "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\r\n",
      "    \r\n",
      "    You can also pipe the output to stdout, rather than a file\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb --stdout\r\n",
      "    \r\n",
      "    PDF is generated via latex\r\n",
      "    \r\n",
      "    > jupyter nbconvert mynotebook.ipynb --to pdf\r\n",
      "    \r\n",
      "    You can get (and serve) a Reveal.js-powered slideshow\r\n",
      "    \r\n",
      "    > jupyter nbconvert myslides.ipynb --to slides --post serve\r\n",
      "    \r\n",
      "    Multiple notebooks can be given at the command line in a couple of \r\n",
      "    different ways:\r\n",
      "    \r\n",
      "    > jupyter nbconvert notebook*.ipynb\r\n",
      "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\r\n",
      "    \r\n",
      "    or you can specify the notebooks list in a config file, containing::\r\n",
      "    \r\n",
      "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\r\n",
      "    \r\n",
      "    > jupyter nbconvert --config mycfg.py\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Keras_Character_MultiHeadRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 speeches for 14 presidents.\n",
      "Loaded 1764 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "    \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "    \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "    \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "    \"Ford\": \"Gerald R. Ford\",\n",
    "    \"Truman\": \"Harry S. Truman\",\n",
    "    \"Hoover\": \"Herbert Hoover\",\n",
    "    \"Carter\": \"Jimmy Carter\",\n",
    "    \"Kennedy\": \"John F. Kennedy\",\n",
    "    \"Johnson\": \"Lyndon B. Johnson\",\n",
    "    \"Nixon\": \"Richard Nixon\",\n",
    "    \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    "     \n",
    "    # REPLACE WORD IN ALL CAPS with <space>; headers\n",
    "    loaded_text[x] = re.sub('[A-Z]{2,}','', loaded_text[x])\n",
    "\n",
    "print \"Replacements complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4862045\n",
      "1  : Donald J. Trump      \t554978\n",
      "2  : Dwight D. Eisenhower \t3084215\n",
      "3  : Franklin D. Roosevelt \t1743457\n",
      "4  : George Bush          \t1896536\n",
      "5  : George W. Bush       \t1791014\n",
      "6  : Gerald R. Ford       \t687272\n",
      "7  : Harry S. Truman      \t2001118\n",
      "8  : Herbert Hoover       \t786184\n",
      "9  : Jimmy Carter         \t1272152\n",
      "10 : John F. Kennedy      \t1344012\n",
      "11 : Lyndon B. Johnson    \t2288606\n",
      "12 : Richard Nixon        \t972275\n",
      "13 : Ronald Reagan        \t1010291\n",
      "14 : William J. Clinton   \t1784166\n",
      "\n",
      "Minimum number of characters per president?\n",
      "554978\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4725447, ' '), (2569974, 'e'), (2131220, 't'), (1627649, 'o'), (1609954, 'a'), (1476541, 'n'), (1405067, 'i'), (1193120, 's'), (1157275, 'r'), (1066798, 'h'), (755187, 'l'), (705084, 'd'), (548669, 'u'), (534278, 'c'), (478832, 'm'), (418343, 'w'), (393337, 'g'), (389784, 'f'), (382020, 'p'), (374441, 'y'), (281512, 'b'), (264600, ','), (263713, '.'), (240856, 'v'), (162701, 'I'), (151063, 'k'), (95930, \"'\"), (62618, 'A'), (49111, 'S'), (47587, 'T'), (44390, '-'), (40548, '7'), (39451, 'W'), (32811, 'x'), (29785, 'C'), (25503, 'j'), (23659, 'B'), (23374, 'M'), (22167, 'N'), (21726, 'q'), (21723, 'P'), (16943, 'G'), (14374, 'H'), (13620, 'U'), (12142, 'D'), (11930, 'z'), (11837, 'R'), (10677, 'F'), (10672, '\"'), (10127, 'E'), (9435, 'O'), (9370, 'Y'), (9360, ';'), (8141, '?'), (7920, 'J'), (7822, 'L'), (6390, ':'), (4737, '\\n'), (3981, '$'), (3943, 'K'), (3705, 'V'), (1853, '/'), (1066, 'Q'), (338, 'Z'), (80, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 1043126 1043126\n",
      "\n",
      "Total characters: 26078150\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 25\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples\n",
    "#\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "        \n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "        \n",
    "    return train_text,train_labels,test_text,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      " Test =  834494 \n",
      " Train =  208632\n",
      "Class weights:\n",
      "{0: 0.35757490058960645, 1: 3.1326613735758393, 2: 0.5636911396167278, 3: 0.9971846806476669, 4: 0.916704016170138, 5: 0.9707030522985297, 6: 2.5296895840911846, 7: 0.8687894640951563, 8: 2.2114295557233903, 9: 1.3666339130719596, 10: 1.2935484871031746, 11: 0.7596495300516601, 12: 1.7881503385617554, 13: 1.7208900437185515, 14: 0.9744435881267661}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.8)\n",
    "print \"Splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"Class weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (834494, 25)\n",
      "...to  (20862350, 68)\n",
      "...and reshaping to  (834494, 25, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (208632, 25)\n",
      "...to  (5215800, 68)\n",
      "...and reshaping to  (208632, 25, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = np.array(to_categorical(train_y))\n",
    "test_y = np.array(to_categorical(test_y))\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom activation from Bagnall 2015\n",
    "#  does not appear to perform as well as a ReLU\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def ReSQRT(x):\n",
    "    cond = tf.less_equal(x, 0.0)\n",
    "    result = tf.where(cond, x * 0.0, tf.sqrt(x+1)-1)\n",
    "    return result\n",
    "\n",
    "get_custom_objects().update({'ReSQRT': ReSQRT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bagnall proposes that the following possible values contribute to the success of the model:\n",
    "\n",
    "| meta-parameter                  \t| typical value                      \t|\n",
    "|---------------------------------\t|------------------------------------\t|\n",
    "| initial adagrad learning scale  \t| 0.1, 0.14, 0.2, 0.3                \t|\n",
    "| initial leakage between classes \t| 1/4N to 5/N                        \t|\n",
    "| leakage decay (per sub-epoch)   \t| 0.67 to 0.9                        \t|\n",
    "| hidden neurons                  \t| 79, 99, 119, 139                   \t|\n",
    "| presynaptic noise σ             \t| 0, 0.1, 0.2, 0.3, 0.5              \t|\n",
    "| sub-epochs                      \t| 6 to 36                            \t|\n",
    "| text direction                  \t| forward or backward                \t|\n",
    "| text handling                   \t| sequential, concatenated, balanced \t|\n",
    "| initialisation                  \t| gaussian, zero                     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 25, 68)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 300)           65700       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_30 (Dense)                 (None, 15)            4515        bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 15)            0           dense_16[0][0]                   \n",
      "                                                                   dense_17[0][0]                   \n",
      "                                                                   dense_18[0][0]                   \n",
      "                                                                   dense_19[0][0]                   \n",
      "                                                                   dense_20[0][0]                   \n",
      "                                                                   dense_21[0][0]                   \n",
      "                                                                   dense_22[0][0]                   \n",
      "                                                                   dense_23[0][0]                   \n",
      "                                                                   dense_24[0][0]                   \n",
      "                                                                   dense_25[0][0]                   \n",
      "                                                                   dense_26[0][0]                   \n",
      "                                                                   dense_27[0][0]                   \n",
      "                                                                   dense_28[0][0]                   \n",
      "                                                                   dense_29[0][0]                   \n",
      "                                                                   dense_30[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 133,425\n",
      "Trainable params: 133,425\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "121100/834494 [===>..........................] - ETA: 456s - loss: 2.6951 - categorical_accuracy: 0.0738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-40444978af04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m           verbose=1)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keras_Character_MultiHeadRNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.layers.merge import Maximum, Add, Concatenate\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.merge import Average, Maximum\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define operating vars\n",
    "activation = \"relu\" #\"ReSQRT\" \n",
    "units = 150# 50\n",
    "dropout = 0.7646166765488501\n",
    "batch_size = 50# 100\n",
    "epochs = 100\n",
    "optimizer='adamax'#'rmsprop'\n",
    "shuffle=True #False\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
    "              patience=2, verbose=1)\n",
    "csv_logger = CSVLogger('Keras_Character_MultiHeadRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=units,activation=activation))(input)\n",
    "\n",
    "## not entirely sure this makes sense, but it does seem to work...\n",
    "soft_out = []\n",
    "for idx in range(0,len(labels)):\n",
    "    soft_out.append( Dense(len(labels),activation='softmax', kernel_initializer='random_normal')(rnn) )\n",
    "final_out = Add()(soft_out)\n",
    "\n",
    "model = Model(inputs=[input], outputs = final_out) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "plot_model(model, to_file='Keras_Character_MultiHeadRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=1)\n",
    "\n",
    "model.save('Keras_Character_MultiHeadRNN.h5')\n",
    "print (\"Model saved.\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_Character_MultiHeadRNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        print(np.sum(cm,axis=0))\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(np.round(cnf_matrix,2), classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
