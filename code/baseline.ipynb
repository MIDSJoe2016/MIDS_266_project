{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires NLTK, BeautifulSoup\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data from ../data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/processed/ ...\n",
      "Donald J. Trump: vocab count 14160, sentence count 4385, word count 67437\n",
      "Barack Obama: vocab count 15089, sentence count 3106, word count 67149\n",
      "Abraham Lincoln: vocab count 15977, sentence count 2102, word count 66180\n"
     ]
    }
   ],
   "source": [
    "reload(load_data)\n",
    "pres_dict = load_data.read_processed_data(\"../data/processed/\")\n",
    "load_data.print_dict(pres_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse json data from ../data/unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/unprocessed/ ...\n",
      "../data/unprocessed/Presidential_News_Confs_(1929-2017).json\n",
      "../data/unprocessed/Presidential_News_Confs_(2009-).json\n",
      "Lyndon B. Johnson: vocab count 118961, sentence count 22235, word count 445978\n",
      "Gerald R. Ford: vocab count 34320, sentence count 6283, word count 135621\n",
      "Richard Nixon: vocab count 40528, sentence count 7426, word count 191118\n",
      "Franklin D. Roosevelt: vocab count 123191, sentence count 16084, word count 399627\n",
      "William J. Clinton: vocab count 73996, sentence count 16625, word count 359557\n",
      "Harry S. Truman: vocab count 139863, sentence count 31957, word count 413259\n",
      "Abraham Lincoln: vocab count 15977, sentence count 2102, word count 66180\n",
      "George Bush: vocab count 87076, sentence count 21374, word count 385669\n",
      "John F. Kennedy: vocab count 61138, sentence count 11051, word count 258231\n",
      "Dwight D. Eisenhower: vocab count 161616, sentence count 22835, word count 601774\n",
      "Ronald Reagan: vocab count 48017, sentence count 8770, word count 192872\n",
      "George W. Bush: vocab count 71606, sentence count 20821, word count 361516\n",
      "Herbert Hoover: vocab count 74728, sentence count 3027, word count 103986\n",
      "Barack Obama: vocab count 203287, sentence count 38956, word count 952769\n",
      "Donald J. Trump: vocab count 19622, sentence count 7003, word count 104519\n",
      "Jimmy Carter: vocab count 61404, sentence count 10961, word count 244037\n"
     ]
    }
   ],
   "source": [
    "load_data.read_unprocessed_data(pres_dict, \"../data/unprocessed/\")\n",
    "load_data.print_dict(pres_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up train and test data\n",
    "This part of code will use num_of_words and the threshold of words to select from each president speeches. If a president doesn't have word counts (less than num_of_words), he will be skipped.\n",
    "\n",
    "The data will be converted into 2d matrix give a batch size, and then split into 80% for training and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words: 100000\n",
      "Processing data for Lyndon B. Johnson\n",
      "Processing data for Gerald R. Ford\n",
      "Processing data for Richard Nixon\n",
      "Processing data for Franklin D. Roosevelt\n",
      "Processing data for William J. Clinton\n",
      "Processing data for Harry S. Truman\n",
      "Processing data for George Bush\n",
      "Processing data for John F. Kennedy\n",
      "Processing data for Dwight D. Eisenhower\n",
      "Processing data for Ronald Reagan\n",
      "Processing data for George W. Bush\n",
      "Processing data for Herbert Hoover\n",
      "Processing data for Barack Obama\n",
      "Processing data for Donald J. Trump\n",
      "Processing data for Jimmy Carter\n",
      "(12000, 100) (12000, 15) (3000, 100) (3000, 15)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_of_words = 100000\n",
    "president_int, y_train, X_train, y_test, X_test = load_data.create_train_test_data(pres_dict, num_of_words, batch_size)\n",
    "\n",
    "# X_train.shape = (1407, 100): the number of original words were 140700. They were broken into batches of 100\n",
    "# y_train.shape = (1407, 4): each batch is mapped to 1 of the 4 presidents\n",
    "print X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Keras Model as a baseline and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         10000000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                765       \n",
      "=================================================================\n",
      "Total params: 10,030,965\n",
      "Trainable params: 10,030,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training...\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 32s - loss: 8.3036 - acc: 0.0670    \n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2700 - acc: 0.0667    \n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2323 - acc: 0.0667    \n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.1222 - acc: 0.0667    \n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2283 - acc: 0.0667    \n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2914 - acc: 0.0667    \n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 32s - loss: 8.1410 - acc: 0.0667    \n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2525 - acc: 0.0667    \n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2525 - acc: 0.0667    \n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 31s - loss: 8.2525 - acc: 0.0667    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x158ed8fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Instantiate and build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_of_words, batch_size))\n",
    "model.add(LSTM(50, input_shape=(None,batch_size)))\n",
    "model.add(Dense(units=len(president_int.keys())))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=1000, input_dim=batch_size))\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Dense(units=1000, input_dim=200))\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Dense(units=1000, input_dim=100))\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Dense(units=len(president_int.keys())))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# Compile w/ chosen loss, optimization fns; specific output metrics\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='Adagrad',\n",
    "#               metrics=['categorical_accuracy','accuracy'])\n",
    "# Train\n",
    "print \"Training...\"\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test data...\n",
      "2900/3000 [============================>.] - ETA: 0s\n",
      "\n",
      "Predicting using test data...\n",
      "\n",
      "\n",
      "['loss', 'acc']\n",
      "[7.9569677352905277, 0.066666666666666666]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\n\\nPredicting using test data...\"\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "#print (classes)\n",
    "\n",
    "print \"\\n\\n\",model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "\n",
    "def prediction_to_01(predictions):\n",
    "    max_p = predictions.max(axis=1)\n",
    "\n",
    "    pred_int = None\n",
    "    for i in range(len(max_p)):\n",
    "        pred_int = load_data.append_matrices(pred_int, [(predictions[i] == max_p[i]).astype(int)])\n",
    "    return pred_int\n",
    "\n",
    "print prediction_to_01(predictions)\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
