{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1750 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "labels = {\"Barack Obama\": 0,\n",
    "          \"Donald J. Trump\": 1,\n",
    "          \"Dwight D. Eisenhower\": 2,\n",
    "          \"Franklin D. Roosevelt\": 3,\n",
    "          \"George Bush\": 4,\n",
    "          \"George W. Bush\": 5,\n",
    "          \"Gerald R. Ford\": 6,\n",
    "          \"Harry S. Truman\": 7,\n",
    "          \"Herbert Hoover\": 8,\n",
    "          \"Jimmy Carter\": 9,\n",
    "          \"John F. Kennedy\": 10,\n",
    "          \"Lyndon B. Johnson\": 11,\n",
    "          \"Richard Nixon\": 12,\n",
    "          \"Ronald Reagan\": 13,\n",
    "          \"William J. Clinton\": 14}\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "directory = \"../data/processed/\"\n",
    "file_to_label = {\"Obama\": \"Barack Obama\", \"Trump\": \"Donald J. Trump\"}\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "    raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "    loaded_text = loaded_text + [raw] \n",
    "        \n",
    "# load JSON text files; parsing into raw text\n",
    "directory = \"../data/unprocessed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.json')):\n",
    "        json_data=open(filename)\n",
    "        data = json.load(json_data)\n",
    "        json_data.close()\n",
    "        for data2 in data[\"speeches\"]:\n",
    "            if ('News Conference With' not in data2['name']):\n",
    "                # data2['text'] has a lot of htmtl tags in there. We still need to parse it            \n",
    "                raw = BeautifulSoup(data2['text'], \"html.parser\").get_text(\" \")\n",
    "                raw = unicodedata.normalize('NFKD', raw).encode('ascii','ignore')\n",
    "                # Remove []\n",
    "                raw = re.sub(' \\[.*?\\]',' ', raw, flags=re.DOTALL)\n",
    "                # Remove ()\n",
    "                raw = re.sub(' \\(.*?\\)',' ', raw, flags=re.DOTALL)\n",
    "                # Removing the questions\n",
    "                raw = re.sub('[A-Z,\\s,\\.]Q\\..*? The President\\.','\\.',raw, flags=re.DOTALL)\n",
    "                raw = re.sub('^[A-Z,\\s]*THE PRESIDENT\\.','',raw, flags=re.DOTALL)\n",
    "                raw = re.sub('[A-Z,\\s,\\.]Q\\..*?THE PRESIDENT\\.','\\.',raw, flags=re.DOTALL)\n",
    "                \n",
    "                # capture speaker (i.e., label)\n",
    "                speaker = data2['speaker']\n",
    "                \n",
    "                # push speaker and text\n",
    "                if (load_verbose == 1):\n",
    "                    print \"Loading: \", data2['speaker'], \"(\", labels[speaker], \"), \", data2['name']\n",
    "                loaded_labels = loaded_labels + [labels[speaker]] \n",
    "                loaded_text = loaded_text + [raw]\n",
    "\n",
    "# input_text = loaded_text\n",
    "# input_labels = loaded_labels\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many speeches per president?\n",
      "0  : Barack Obama         \t152\n",
      "1  : Donald J. Trump      \t19\n",
      "2  : Dwight D. Eisenhower \t192\n",
      "3  : Franklin D. Roosevelt \t223\n",
      "4  : George Bush          \t97\n",
      "5  : George W. Bush       \t54\n",
      "6  : Gerald R. Ford       \t40\n",
      "7  : Harry S. Truman      \t301\n",
      "8  : Herbert Hoover       \t267\n",
      "9  : Jimmy Carter         \t59\n",
      "10 : John F. Kennedy      \t63\n",
      "11 : Lyndon B. Johnson    \t134\n",
      "12 : Richard Nixon        \t39\n",
      "13 : Ronald Reagan        \t46\n",
      "14 : William J. Clinton   \t64\n",
      "\n",
      "Approximately many words of text per president?\n",
      "0  : Barack Obama         \t899079\n",
      "1  : Donald J. Trump      \t87764\n",
      "2  : Dwight D. Eisenhower \t579577\n",
      "3  : Franklin D. Roosevelt \t392144\n",
      "4  : George Bush          \t354916\n",
      "5  : George W. Bush       \t322439\n",
      "6  : Gerald R. Ford       \t127862\n",
      "7  : Harry S. Truman      \t397252\n",
      "8  : Herbert Hoover       \t164674\n",
      "9  : Jimmy Carter         \t226731\n",
      "10 : John F. Kennedy      \t243153\n",
      "11 : Lyndon B. Johnson    \t425941\n",
      "12 : Richard Nixon        \t178999\n",
      "13 : Ronald Reagan        \t180653\n",
      "14 : William J. Clinton   \t332077\n",
      "\n",
      "Approximately how many average words per speech per president?\n",
      "0  : Barack Obama         \t5914\n",
      "1  : Donald J. Trump      \t4619\n",
      "2  : Dwight D. Eisenhower \t3018\n",
      "3  : Franklin D. Roosevelt \t1758\n",
      "4  : George Bush          \t3658\n",
      "5  : George W. Bush       \t5971\n",
      "6  : Gerald R. Ford       \t3196\n",
      "7  : Harry S. Truman      \t1319\n",
      "8  : Herbert Hoover       \t616\n",
      "9  : Jimmy Carter         \t3842\n",
      "10 : John F. Kennedy      \t3859\n",
      "11 : Lyndon B. Johnson    \t3178\n",
      "12 : Richard Nixon        \t4589\n",
      "13 : Ronald Reagan        \t3927\n",
      "14 : William J. Clinton   \t5188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# summary stats & chop up into smaller\n",
    "#print \"Loaded\", len(input_text), \"speeches for\", len(set(input_labels)), \"presidents.\"\n",
    "\n",
    "print \"\\nHow many speeches per president?\"\n",
    "speech_freq = np.bincount(loaded_labels)\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", speech_freq[value]\n",
    "  \n",
    "print \"\\nApproximately many words of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    label_set = [cnt for cnt, idx in enumerate(loaded_labels) if idx == value]\n",
    "    label_speeches = [loaded_text[i] for i in label_set]\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", sum(len(speech.split()) for speech in label_speeches)\n",
    "\n",
    "print \"\\nApproximately how many average words per speech per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    label_set = [cnt for cnt, idx in enumerate(loaded_labels) if idx == value]\n",
    "    label_speeches = [loaded_text[i] for i in label_set]\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", (sum(len(speech.split()) for speech in label_speeches)) / speech_freq[value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed  303108 sentences, applying 303108 labels.\n",
      "\n",
      "Approximately many sentences of text per president?\n",
      "0  : Barack Obama         \t46115\n",
      "1  : Donald J. Trump      \t7436\n",
      "2  : Dwight D. Eisenhower \t30381\n",
      "3  : Franklin D. Roosevelt \t26401\n",
      "4  : George Bush          \t24820\n",
      "5  : George W. Bush       \t22448\n",
      "6  : Gerald R. Ford       \t7455\n",
      "7  : Harry S. Truman      \t43005\n",
      "8  : Herbert Hoover       \t7595\n",
      "9  : Jimmy Carter         \t12174\n",
      "10 : John F. Kennedy      \t12436\n",
      "11 : Lyndon B. Johnson    \t25931\n",
      "12 : Richard Nixon        \t8379\n",
      "13 : Ronald Reagan        \t10544\n",
      "14 : William J. Clinton   \t17988\n",
      "\n",
      "Summary stats of sentence counts\n",
      "DescribeResult(nobs=15, minmax=(7436, 46115), mean=20207.200000000001, variance=159986167.02857143, skewness=0.7711964435384968, kurtosis=-0.4618739369874838)\n",
      "\n",
      "Maximum sentence length (characters) =  1364\n",
      "And throughout this process, based on hours of meetingsif you tallied it up, days or weeks of meetings where we went through every option in painful detail, with maps, and we had our military, and we had our aid agencies, and we had our diplomatic teams, and sometimes, we'd bring in outsiders who were critics of ourswhenever we went through it, the challenge was that, short of putting large numbers of U.S. troops on the ground, uninvited, without any international law mandate, without sufficient support from Congress, at a time when we still had troops in Afghanistan and we still had troops in Iraq and we had just gone through over a decade of war and spent trillions of dollars, and when the opposition on the ground was not cohesive enough to necessarily govern a country, and you had a military superpower in Russia prepared to do whatever it took to keep its client state involved, and you had a regional military power in Iran that saw their own vital strategic interests at stake and were willing to send in as many of their people or proxies to support the regimethat in that circumstance, unless we were all in and willing to take over Syria, we were going to have problems, and that everything else was tempting because we wanted to do something and it sounded like the right thing to do, but it was going to be impossible to do this on the cheap.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "from scipy import stats\n",
    "\n",
    "# parse speeches into sentences and see what we have\n",
    "input_text = []\n",
    "input_labels = []\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for idx in range(0,len(loaded_text)):\n",
    "    speech = loaded_text[idx]\n",
    "    label = loaded_labels[idx]\n",
    "    parsed_sentences = sent_detector.tokenize(speech.strip())\n",
    "    input_text = input_text + parsed_sentences\n",
    "    input_labels = input_labels + ([label]*len(parsed_sentences))\n",
    "\n",
    "print \"Parsed \", len(input_text), \"sentences, applying\", len(input_labels), \"labels.\"\n",
    "\n",
    "print \"\\How many sentences of text per president?\"\n",
    "sentence_label_count = np.bincount(input_labels)\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", sentence_label_count[value]\n",
    "\n",
    "print \"\\nSummary stats of sentence counts\"\n",
    "print stats.describe(sentence_label_count)\n",
    "\n",
    "max_sentence_len = len(max(input_text, key=len))\n",
    "print \"\\nMaximum sentence length (characters) = \", max_sentence_len\n",
    "print max(input_text, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : Barack Obama         \ttrim \t46115  now at: 7500\n",
      "1  : Donald J. Trump      \tcopy \t7436   now at: 7436\n",
      "2  : Dwight D. Eisenhower \ttrim \t30381  now at: 7500\n",
      "3  : Franklin D. Roosevelt \ttrim \t26401  now at: 7500\n",
      "4  : George Bush          \ttrim \t24820  now at: 7500\n",
      "5  : George W. Bush       \ttrim \t22448  now at: 7500\n",
      "6  : Gerald R. Ford       \tcopy \t7455   now at: 7455\n",
      "7  : Harry S. Truman      \ttrim \t43005  now at: 7500\n",
      "8  : Herbert Hoover       \ttrim \t7595   now at: 7500\n",
      "9  : Jimmy Carter         \ttrim \t12174  now at: 7500\n",
      "10 : John F. Kennedy      \ttrim \t12436  now at: 7500\n",
      "11 : Lyndon B. Johnson    \ttrim \t25931  now at: 7500\n",
      "12 : Richard Nixon        \ttrim \t8379   now at: 7500\n",
      "13 : Ronald Reagan        \ttrim \t10544  now at: 7500\n",
      "14 : William J. Clinton   \ttrim \t17988  now at: 7500\n",
      "\n",
      "Sentences trimmed from 303108 to 112391\n",
      "\n",
      "Labels trimmed from 303108 to 112391\n"
     ]
    }
   ],
   "source": [
    "# adjust sentence volumes \n",
    "from operator import itemgetter \n",
    "\n",
    "# approach here is too simplistic but it suffices for now:\n",
    "#   If <= threshold, take all; else just pick first threshold # of sentences sentences\n",
    "\n",
    "sentence_max_threshold = 7500\n",
    "\n",
    "trimmed_text = []\n",
    "trimmed_labels = []\n",
    "sentence_label_count = np.bincount(input_labels)\n",
    "\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    # grab all values of a specific label\n",
    "    subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "    subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "\n",
    "    if sentence_label_count[value] <= sentence_max_threshold:\n",
    "        print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", \"copy\", \"\\t\", str(sentence_label_count[value]).ljust(6), \"now at:\", len(subset_text)\n",
    "    else:\n",
    "        subset_text = subset_text[0:sentence_max_threshold]\n",
    "        subset_labels = subset_labels[0:sentence_max_threshold]\n",
    "        print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", \"trim\", \"\\t\", str(sentence_label_count[value]).ljust(6), \"now at:\", len(subset_text)\n",
    "    trimmed_text = trimmed_text + subset_text\n",
    "    trimmed_labels = trimmed_labels + subset_labels\n",
    "\n",
    "# free up some memory\n",
    "subset_labels = None\n",
    "subset_text = None\n",
    "\n",
    "print \"\\nSentences trimmed from\", len(input_text), \"to\", len(trimmed_text)\n",
    "print \"\\nLabels trimmed from\", len(input_labels), \"to\", len(trimmed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training ( 89912 records) and test ( 22479 records) data sets.\n"
     ]
    }
   ],
   "source": [
    "## USE NLTK Tokenizer instead?\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_words = 15000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(trimmed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(trimmed_text)\n",
    "\n",
    "X = sequence.pad_sequences(tokenized_text, maxlen=max_sentence_len)\n",
    "y = to_categorical(trimmed_labels)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state=45)\n",
    "\n",
    "print \"Prepared training (\", len(train_X), \"records) and test (\", len(test_X), \"records) data sets.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1364, 100)         1500000   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1364, 100)         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_9 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 15)                1515      \n",
      "=================================================================\n",
      "Total params: 1,521,615\n",
      "Trainable params: 1,521,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "22680/89912 [======>.......................] - ETA: 976s - loss: 2.6625 - categorical_accuracy: 0.1125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-578828dc7dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, SimpleRNN, Dropout\n",
    "\n",
    "max_features = 15000\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100, input_length=max_sentence_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adagrad',metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_X, y=train_y, batch_size=40, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=100)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from scikit-learn examples @\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
