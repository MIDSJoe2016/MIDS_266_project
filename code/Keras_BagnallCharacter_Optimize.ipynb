{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "### THIS IS ONLY FOR HYPERPARAMETER TUNING\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout, Activation\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam, sgd, rmsprop\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## THIS WON'T RUN UNLESS YOU HAVE THE (HUGE) SAVED PICKLE FILES FROM THE CHARACTER MODEL\n",
    "import pickle\n",
    "\n",
    "def data():\n",
    "    pkl_file = open('test_X.pkl', 'rb')\n",
    "    X_test = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print (\"test_X loaded\")\n",
    "\n",
    "    pkl_file = open('test_y.pkl', 'rb')\n",
    "    Y_test = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print (\"test_y loaded\")\n",
    "\n",
    "    pkl_file = open('train_X.pkl', 'rb')\n",
    "    X_train = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print (\"train_X loaded\")\n",
    "\n",
    "    pkl_file = open('train_y.pkl', 'rb')\n",
    "    Y_train = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print (\"train_y loaded\")\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.4189557794750016, 1: 3.6705746738641474, 2: 2.1043681495809157, 3: 1.1554357830642878}\n"
     ]
    }
   ],
   "source": [
    "y_weights = np.load('y_weights.npy').item()\n",
    "print (y_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X loaded\n",
      "test_y loaded\n",
      "train_X loaded\n",
      "train_y loaded\n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adagrad, adam, sgd, rmsprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'units': hp.choice('units', [25, 50, 100, 150]),\n",
      "        'activation': hp.choice('activation', ['relu', 'tanh']),\n",
      "        'merge_mode': hp.choice('merge_mode', ['ave', 'sum', 'mul', 'concat']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [25, 50, 100, 250, 500]),\n",
      "        'shuffle': hp.choice('shuffle', [True, False]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: pkl_file = open('test_X.pkl', 'rb')\n",
      "  3: X_test = pickle.load(pkl_file)\n",
      "  4: pkl_file.close()\n",
      "  5: print (\"test_X loaded\")\n",
      "  6: \n",
      "  7: pkl_file = open('test_y.pkl', 'rb')\n",
      "  8: Y_test = pickle.load(pkl_file)\n",
      "  9: pkl_file.close()\n",
      " 10: print (\"test_y loaded\")\n",
      " 11: \n",
      " 12: pkl_file = open('train_X.pkl', 'rb')\n",
      " 13: X_train = pickle.load(pkl_file)\n",
      " 14: pkl_file.close()\n",
      " 15: print (\"train_X loaded\")\n",
      " 16: \n",
      " 17: pkl_file = open('train_y.pkl', 'rb')\n",
      " 18: Y_train = pickle.load(pkl_file)\n",
      " 19: pkl_file.close()\n",
      " 20: print (\"train_y loaded\")\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      " 24: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     # assemble & compile model\n",
      "   5:     model = Sequential()\n",
      "   6:     model.add(Dense(68,input_shape=(50,68,)))\n",
      "   7:     model.add(Bidirectional(SimpleRNN(units=space['units'],\n",
      "   8:                                       activation=space['activation']),\n",
      "   9:                             merge_mode=space['merge_mode']\n",
      "  10:                            ))\n",
      "  11:     model.add(Dropout(space['Dropout']))\n",
      "  12:     model.add(Dense(4))\n",
      "  13:     model.add(Activation('softmax'))\n",
      "  14: \n",
      "  15:     model.compile(loss='categorical_crossentropy', \n",
      "  16:                   optimizer=space['optimizer'], \n",
      "  17:                   metrics=['categorical_accuracy'])\n",
      "  18:     # train\n",
      "  19:     model.fit(X_train, Y_train,\n",
      "  20:               batch_size=space['batch_size'],\n",
      "  21:               epochs=3,\n",
      "  22:               shuffle=space['shuffle'],\n",
      "  23:               class_weight = {0: 0.4189557794750016, 1: 3.6705746738641474, 2: 2.1043681495809157, 3: 1.1554357830642878},\n",
      "  24:               validation_data=(X_test, Y_test),\n",
      "  25:               verbose=1)\n",
      "  26:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  27:     print('\\n\\n') #Test accuracy:', acc)\n",
      "  28:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  29: \n",
      "test_X loaded\n",
      "test_y loaded\n",
      "train_X loaded\n",
      "train_y loaded\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 70s - loss: 0.6844 - categorical_accuracy: 0.9464 - val_loss: 10.1897 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 61s - loss: 1.1352 - categorical_accuracy: 0.8417 - val_loss: 5.5091 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 62s - loss: 0.5157 - categorical_accuracy: 0.9202 - val_loss: 8.9142 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 50s - loss: 1.4689 - categorical_accuracy: 0.2143 - val_loss: 1.3852 - val_categorical_accuracy: 0.2891\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 71s - loss: 1.3876 - categorical_accuracy: 0.1938 - val_loss: 1.3792 - val_categorical_accuracy: 0.5552\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 77s - loss: 1.3872 - categorical_accuracy: 0.2703 - val_loss: 1.3856 - val_categorical_accuracy: 0.2412\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 58s - loss: 1.3834 - categorical_accuracy: 0.2398 - val_loss: 1.3583 - val_categorical_accuracy: 0.4473\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 55s - loss: 1.3797 - categorical_accuracy: 0.2465 - val_loss: 1.3011 - val_categorical_accuracy: 0.5014\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 40s - loss: 1.3628 - categorical_accuracy: 0.2601 - val_loss: 1.4542 - val_categorical_accuracy: 0.1352\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 136s - loss: 12.0891 - categorical_accuracy: 0.5949 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 129s - loss: 12.0895 - categorical_accuracy: 0.5954 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 129s - loss: 12.0916 - categorical_accuracy: 0.5960 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 101s - loss: 1.4128 - categorical_accuracy: 0.2144 - val_loss: 1.3851 - val_categorical_accuracy: 0.2965\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 100s - loss: 1.3856 - categorical_accuracy: 0.3196 - val_loss: 1.3838 - val_categorical_accuracy: 0.2948\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 98s - loss: 1.3946 - categorical_accuracy: 0.3029 - val_loss: 1.3765 - val_categorical_accuracy: 0.3538\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 61s - loss: 1.3069 - categorical_accuracy: 0.6758 - val_loss: 1.5294 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 60s - loss: 1.3374 - categorical_accuracy: 0.4334 - val_loss: 1.5787 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 61s - loss: 1.3502 - categorical_accuracy: 0.3685 - val_loss: 1.5932 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 190s - loss: 12.0885 - categorical_accuracy: 0.5961 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 193s - loss: 12.0907 - categorical_accuracy: 0.5964 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 186s - loss: 12.0882 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 74s - loss: 12.0890 - categorical_accuracy: 0.5963 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 74s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 71s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 26s - loss: 0.5516 - categorical_accuracy: 0.8799 - val_loss: 3.7188 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 26s - loss: 0.6363 - categorical_accuracy: 0.8756 - val_loss: 3.3092 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 27s - loss: 0.5394 - categorical_accuracy: 0.8949 - val_loss: 4.3635 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130555/130555 [==============================] - 151s - loss: 1.3886 - categorical_accuracy: 0.2264 - val_loss: 1.3860 - val_categorical_accuracy: 0.2048\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 152s - loss: 1.3843 - categorical_accuracy: 0.2362 - val_loss: 1.3882 - val_categorical_accuracy: 0.1802\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 150s - loss: 1.3817 - categorical_accuracy: 0.2293 - val_loss: 1.3793 - val_categorical_accuracy: 0.2420\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 101s - loss: 12.0890 - categorical_accuracy: 0.5960 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 102s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 102s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 34s - loss: 1.3737 - categorical_accuracy: 0.2194 - val_loss: 1.3948 - val_categorical_accuracy: 0.1429\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 34s - loss: 1.3373 - categorical_accuracy: 0.1953 - val_loss: 1.3659 - val_categorical_accuracy: 0.1822\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 34s - loss: 1.3178 - categorical_accuracy: 0.2198 - val_loss: 1.4791 - val_categorical_accuracy: 0.1687\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 97s - loss: 12.0897 - categorical_accuracy: 0.5954 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 101s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 108s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 31s - loss: 1.5077 - categorical_accuracy: 0.7013 - val_loss: 12.6303 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 28s - loss: 1.8102 - categorical_accuracy: 0.7111 - val_loss: 2.7986 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 29s - loss: 1.4628 - categorical_accuracy: 0.8349 - val_loss: 4.7613 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 97s - loss: 2.8182 - categorical_accuracy: 0.9431 - val_loss: 12.6304 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 97s - loss: 6.4873 - categorical_accuracy: 0.8473 - val_loss: 12.4922 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 93s - loss: 0.4836 - categorical_accuracy: 0.9780 - val_loss: 11.9309 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 150s - loss: 3.4365 - categorical_accuracy: 0.9335 - val_loss: 12.6304 - val_categorical_accuracy: 0.2164\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 155s - loss: 1.0339 - categorical_accuracy: 0.9645 - val_loss: 12.6303 - val_categorical_accuracy: 0.2164\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 152s - loss: 10.5879 - categorical_accuracy: 0.6698 - val_loss: 12.6304 - val_categorical_accuracy: 0.2164\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 101s - loss: 12.0908 - categorical_accuracy: 0.5955 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 127s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 123s - loss: 12.0886 - categorical_accuracy: 0.5967 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 192s - loss: 7.8038 - categorical_accuracy: 0.4604 - val_loss: 6.4712 - val_categorical_accuracy: 0.5985\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 195s - loss: 12.0375 - categorical_accuracy: 0.5950 - val_loss: 6.4732 - val_categorical_accuracy: 0.5984\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 209s - loss: 11.9832 - categorical_accuracy: 0.5925 - val_loss: 6.5804 - val_categorical_accuracy: 0.5917\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 93s - loss: 12.0479 - categorical_accuracy: 0.5946 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 109s - loss: 12.0665 - categorical_accuracy: 0.5950 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 119s - loss: 12.0547 - categorical_accuracy: 0.5965 - val_loss: 6.5004 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "\n",
      "\n",
      "Train on 130555 samples, validate on 32641 samples\n",
      "Epoch 1/3\n",
      "130555/130555 [==============================] - 102s - loss: 1.3767 - categorical_accuracy: 0.2477 - val_loss: 1.4344 - val_categorical_accuracy: 0.2045\n",
      "Epoch 2/3\n",
      "130555/130555 [==============================] - 91s - loss: 1.3360 - categorical_accuracy: 0.3142 - val_loss: 1.3442 - val_categorical_accuracy: 0.2634\n",
      "Epoch 3/3\n",
      "130555/130555 [==============================] - 92s - loss: 1.3355 - categorical_accuracy: 0.3324 - val_loss: 1.3347 - val_categorical_accuracy: 0.5037\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    # assemble & compile model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(68,input_shape=(50,68,)))\n",
    "    model.add(Bidirectional(SimpleRNN(units={{choice([25, 50, 100, 150])}},\n",
    "                                      activation={{choice(['relu', 'tanh'])}}),\n",
    "                            merge_mode={{choice(['ave', 'sum', 'mul', 'concat'])}}\n",
    "                           ))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam'])}}, \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    # train\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([25, 50, 100, 250, 500])}},\n",
    "              epochs=3,\n",
    "              shuffle={{choice([True, False])}},\n",
    "              class_weight = {0: 0.4189557794750016, 1: 3.6705746738641474, 2: 2.1043681495809157, 3: 1.1554357830642878},\n",
    "              validation_data=(X_test, Y_test),\n",
    "              verbose=1)\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('\\n\\n') #Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=20,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Keras_BagnallCharacter_Optimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shuffle': 1, 'merge_mode': 1, 'Dropout': 0.7646166765488501, 'batch_size': 1, 'units': 3, 'optimizer': 5, 'activation': 0}\n"
     ]
    }
   ],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Shorter still better in actual model\n",
    "#\n",
    "# More specific test @ 3 epochs; 20 trials:\n",
    "#{'shuffle': 1, 'merge_mode': 1, 'Dropout': 0.7646166765488501, 'batch_size': 1, 'units': 3, 'optimizer': 5, 'activation': 0}\n",
    "            # optimizer = adamax; \n",
    "            # dropout = 0.7646166765488501; \n",
    "            # batch_size = 50; \n",
    "            # units = 150; \n",
    "            # shuffle = False; \n",
    "            # activation = relu\n",
    "            # merge_mode = 'sum'\n",
    "            \n",
    "# Shorter test @ 3 epochs; 20 trials: \n",
    "# {'optimizer': 0, 'Dropout': 0.2525429012036986, 'batch_size': 1, 'units': 1, 'shuffle': 1, 'activation': 0}\n",
    "            # optimizer = rmsprop; \n",
    "            # dropout = 0.2525429012036986; \n",
    "            # batch_size = 50; \n",
    "            # units = 50; \n",
    "            # shuffle = False; \n",
    "            # activation = relu\n",
    "            \n",
    "# Longer test @ 5 epochs; 50 trials:\n",
    "# {'optimizer': 0, 'Dropout': 0.5422412690636627, 'batch_size': 0, 'units': 3, 'shuffle': 1, 'activation': 0}\n",
    "            # optimizer = rmsprop; \n",
    "            # dropout = 0.5422412690636627; \n",
    "            # batch_size = 25; \n",
    "            # units = 100; \n",
    "            # shuffle = False; \n",
    "            # activation = relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of best performing model:\n",
      "32640/32641 [============================>.] - ETA: 0s[6.5003710097824836, 0.59670353236726814]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
