{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import random as rn\n",
    "rn.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Keras_Character_SimpleRNN.ipynb to script\n",
      "[NbConvertApp] Writing 16969 bytes to Keras_Character_SimpleRNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Keras_Character_SimpleRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 speeches for 4 presidents.\n",
      "Loaded 290 speeches for 4 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "#     \"Dwight D. Eisenhower\",\n",
    "#     \"Franklin D. Roosevelt\",\n",
    "#     \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "#     \"Gerald R. Ford\",\n",
    "#     \"Harry S. Truman\",\n",
    "#     \"Herbert Hoover\",\n",
    "#     \"Jimmy Carter\",\n",
    "#     \"John F. Kennedy\",\n",
    "#     \"Lyndon B. Johnson\",\n",
    "#     \"Richard Nixon\",\n",
    "#     \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "#     \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "#     \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "#     \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "#     \"Ford\": \"Gerald R. Ford\",\n",
    "#     \"Truman\": \"Harry S. Truman\",\n",
    "#     \"Hoover\": \"Herbert Hoover\",\n",
    "#     \"Carter\": \"Jimmy Carter\",\n",
    "#     \"Kennedy\": \"John F. Kennedy\",\n",
    "#     \"Johnson\": \"Lyndon B. Johnson\",\n",
    "#     \"Nixon\": \"Richard Nixon\",\n",
    "#     \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character clean-up complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    " \n",
    "    # REPLACE WORD IN ALL CAPS with <space>; headers\n",
    "    loaded_text[x] = re.sub('[A-Z]{2,}','', loaded_text[x])\n",
    "\n",
    "print \"Character clean-up complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please have a seat, everybody. Good morning. I thought it might make sense to take some questions this week, as my first term comes to an end. It's been a busy and productive 7 years. And I expect the same for the next 7 years. I intend to carry out the agenda that I campaigned on: an agenda for new jobs, new opportunity, and new security for the middle class. Now, right now our economy is growing, and our businesses are creating new jobs, so we are poised for a good year if we make smart decisions and sound investments and as long as Washington politics don't get in the way of America's progress. As I said on the campaign, one component to growing our economy and broadening opportunity for the middle class is shrinking our deficits in a b\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a scrubbed text excerpt\n",
    "print loaded_text[200][:750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4862045\n",
      "1  : Donald J. Trump      \t554978\n",
      "2  : George W. Bush       \t1791014\n",
      "3  : William J. Clinton   \t1784166\n",
      "\n",
      "Minimum number of characters per president?\n",
      "554978\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(1611159, ' '), (879201, 'e'), (725072, 't'), (568823, 'o'), (551892, 'a'), (504754, 'n'), (482361, 'i'), (423814, 's'), (408565, 'r'), (357681, 'h'), (261863, 'l'), (244476, 'd'), (192987, 'u'), (187789, 'c'), (160541, 'm'), (151341, 'g'), (151110, 'w'), (136137, 'p'), (128341, 'y'), (126745, 'f'), (97526, 'b'), (88613, ','), (86646, '.'), (80552, 'v'), (60491, 'k'), (49579, 'I'), (49550, \"'\"), (30038, 'A'), (16842, '-'), (15567, 'S'), (15460, 'T'), (12056, 'W'), (10475, 'x'), (10326, 'j'), (8911, '7'), (8467, 'C'), (8114, 'B'), (7596, 'q'), (7180, 'P'), (5964, 'M'), (5138, 'N'), (5069, 'R'), (4783, 'G'), (4396, 'z'), (4381, 'U'), (4208, 'H'), (3485, '?'), (3371, 'D'), (3331, 'O'), (3272, 'E'), (3056, ';'), (2915, 'F'), (2889, 'Y'), (2803, 'L'), (2752, '\\n'), (2549, '\"'), (2380, 'J'), (1667, 'K'), (1581, '/'), (1447, ':'), (722, '$'), (599, 'V'), (587, 'Q'), (158, 'Z'), (49, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 179842 179842\n",
      "\n",
      "Total characters: 8992100\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 50\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 16, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 16, 7, 6, 16, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 18, 11, 19, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 20, 1, 3, 10, 2, 1, 8, 25], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 20, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 16, 10, 1, 4, 20, 20, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 17, 2, 1, 3, 10, 2, 1, 18, 2, 4, 18, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 20, 5, 7, 3, 10, 20, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 20, 1, 4, 13, 9, 1, 20, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 20, 4, 13, 6, 12, 7, 6, 16, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few split text excerpts\n",
    "print split_text[10:15]\n",
    "print split_labels[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples; shuffle if requested\n",
    "#\n",
    "import sklearn.utils\n",
    "\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8, shuffle_p=True):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "\n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "\n",
    "    if shuffle_p:\n",
    "        test_text, test_labels = sklearn.utils.shuffle(test_text, test_labels)\n",
    "        train_text, train_labels = sklearn.utils.shuffle(train_text, train_labels)\n",
    "\n",
    "    return train_text, train_labels, test_text, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splits:\n",
      " Test =  143873 \n",
      " Train =  35969\n",
      "\n",
      "Class weights:\n",
      "{0: 0.4623643819415878, 1: 4.050934789953824, 2: 1.2551734366275824, 3: 1.260010159041547}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.8, shuffle_p=False)\n",
    "print \"Sample splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"\\nClass weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 16, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 16, 7, 6, 16, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 18, 11, 19, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 20, 1, 3, 10, 2, 1, 8, 25], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 20, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 16, 10, 1, 4, 20, 20, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 17, 2, 1, 3, 10, 2, 1, 18, 2, 4, 18, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 20, 5, 7, 3, 10, 20, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 20, 1, 4, 13, 9, 1, 20, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 20, 4, 13, 6, 12, 7, 6, 16, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few of the split text excerpts; \n",
    "# likely the same classes based on the split point\n",
    "print train_X[10:15]\n",
    "print train_y[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (143873, 50)\n",
      "...to  (7193650, 68)\n",
      "...and reshaping to  (143873, 50, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (35969, 50)\n",
      "...to  (1798450, 68)\n",
      "...and reshaping to  (35969, 50, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "[[ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Have a again look at a few of the split and encoded text excerpts; \n",
    "# both arrays should be one-hot encoded.\n",
    "print train_X[10:11]\n",
    "print train_y[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define operating vars\n",
    "optimizer='rmsprop'\n",
    "dropout = 0.5422412690636627\n",
    "activation = \"relu\"\n",
    "batch_size = 50\n",
    "units = 50\n",
    "shuffle = True\n",
    "epochs = 150\n",
    "merge_mode = 'ave'\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='categorical_accuracy', \n",
    "                              factor=0.5,\n",
    "                              patience=1, \n",
    "                              verbose=1)\n",
    "csv_logger = CSVLogger('Keras_Character_SimpleRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=units,\n",
    "                              activation=activation),\n",
    "                    merge_mode=merge_mode)(main_input)\n",
    "drop = Dropout(dropout)(rnn)\n",
    "main_output = Dense(len(labels),\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='random_normal')(drop)\n",
    "model = Model(inputs=[main_input], outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "plot_model(model, to_file='Keras_Character_SimpleRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=1)\n",
    "\n",
    "model.save('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model saved.\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## MODEL EVALUATION\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model re-loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(np.round(cnf_matrix,2), classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def reverse_map(map):\n",
    "    return dict((v,k) for k,v in map.iteritems())\n",
    "token_word_map = reverse_map(tokenizer.word_index)\n",
    "president_map = reverse_map(labels)\n",
    "def to_words(a):\n",
    "    return \" \".join([token_word_map[id] for id in a if id != 0])\n",
    "sample_size = 20\n",
    "def print_row(row):\n",
    "    print \"\".join(col.ljust(22) for col in row)\n",
    "print_row([\"Predicted\",\"Correct\",\"Sentence\"])\n",
    "print_row([\"----------\",\"----------\",\"---------------\"])\n",
    "sample = []\n",
    "for i in range(len(test_y_collapsed)):\n",
    "    if (pred_y_collapsed[i] != test_y_collapsed[i] and count < sample_size):\n",
    "        sample += [[president_map[pred_y_collapsed[i]], president_map[test_y_collapsed[i]], to_words(test_X[i])]]\n",
    "        count += 1\n",
    "        \n",
    "for row in sample:\n",
    "    print_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "38475/95915 [===========>..................] - ETA: 63s - loss: 1.3191 - categorical_accuracy: 0.3072"
     ]
    }
   ],
   "source": [
    "##\n",
    "## MODEL OPTIMIZATION\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# define operating vars\n",
    "#optimizer='rmsprop'\n",
    "#dropout = 0.5422412690636627\n",
    "#activation = \"relu\"\n",
    "#batch_size = 50\n",
    "#units = 50\n",
    "#shuffle = True\n",
    "#epochs = 150\n",
    "#merge_mode = 'ave'\n",
    "\n",
    "def create_model(optimizer='rmsprop', learn_rate=0.01,\n",
    "                 init_mode1='glorot_uniform', init_mode2='glorot_uniform', \n",
    "                 merge_mode='ave', activation='relu', \n",
    "                 dropout_rate=0.0, neuron_count=50):\n",
    "    # assemble & compile model\n",
    "    main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "    rnn = Bidirectional(SimpleRNN(units=neuron_count,\n",
    "                                  activation=activation,\n",
    "                                  kernel_initializer=init_mode1,\n",
    "                                  recurrent_dropout=dropout_rate),\n",
    "                        merge_mode=merge_mode)(main_input)\n",
    "    main_output = Dense(len(labels),\n",
    "                        activation='softmax',\n",
    "                        kernel_initializer=init_mode2)(rnn)\n",
    "    model = Model(inputs=[main_input], outputs=[main_output])\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1, epochs=5)\n",
    "\n",
    "# define the grid search parameters\n",
    "epoch = [3]\n",
    "batch_sizes = [25, 50, 75, 100, 200]\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "init_modes1 = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "init_modes2 = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "merge_modes = ['sum', 'mul', 'concat', 'ave', None]\n",
    "activations = ['relu','sigmoid','tanh']\n",
    "dropout_rates = [0.0,0.2,0.4,0.6,0.8]\n",
    "neuron_counts = [25,50,75,100,150,200]\n",
    "learn_rates = [0.001, 0.01, 0.1, 0.2, 0.3]  #currently ignored\n",
    "param_grid = dict(batch_size=batch_sizes,\n",
    "                  epochs=epoch,\n",
    "                  optimizer=optimizers,\n",
    "                  learn_rate=learn_rates,\n",
    "                  init_mode1=init_modes1,\n",
    "                  init_mode2=init_modes2,\n",
    "                  merge_mode=merge_modes,\n",
    "                  activation=activations,\n",
    "                  dropout_rate=dropout_rates,\n",
    "                  neuron_count=neuron_counts)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "# summarize results\n",
    "# from http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
