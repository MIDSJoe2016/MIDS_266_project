{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import random as rn\n",
    "rn.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Keras_Character_SimpleRNN.ipynb to script\n",
      "[NbConvertApp] Writing 17210 bytes to Keras_Character_SimpleRNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Keras_Character_SimpleRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 speeches for 14 presidents.\n",
      "Loaded 1764 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "    \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "    \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "    \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "    \"Ford\": \"Gerald R. Ford\",\n",
    "    \"Truman\": \"Harry S. Truman\",\n",
    "    \"Hoover\": \"Herbert Hoover\",\n",
    "    \"Carter\": \"Jimmy Carter\",\n",
    "    \"Kennedy\": \"John F. Kennedy\",\n",
    "    \"Johnson\": \"Lyndon B. Johnson\",\n",
    "    \"Nixon\": \"Richard Nixon\",\n",
    "    \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character clean-up complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    " \n",
    "    # REPLACE WORD IN ALL CAPS with <space>; headers\n",
    "    loaded_text[x] = re.sub('[A-Z]{2,}','', loaded_text[x])\n",
    "\n",
    "print \"Character clean-up complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Before answering questions this evening, I would like to say a few words about the rescue mission in Iran. I share the disappointment of the American people that this rescue mission was not successful, and I also share the grief of our Nation because we had Americans who were casualties in this effort to seek freedom for their fellow citizens who have been held hostage for so long. But I also share a deep pride in the commitment and courage and the integrity and the competence and the determination of those who went on this mission. They were prepared to do their duty, and they did their duty. I can think of no higher compliment for a Commander in Chief to pay to brave men. It was my responsibility as President to launch this mission\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a scrubbed text excerpt\n",
    "print loaded_text[200][:750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4862045\n",
      "1  : Donald J. Trump      \t554978\n",
      "2  : Dwight D. Eisenhower \t3084215\n",
      "3  : Franklin D. Roosevelt \t1743457\n",
      "4  : George Bush          \t1896536\n",
      "5  : George W. Bush       \t1791014\n",
      "6  : Gerald R. Ford       \t687272\n",
      "7  : Harry S. Truman      \t2001118\n",
      "8  : Herbert Hoover       \t786184\n",
      "9  : Jimmy Carter         \t1272152\n",
      "10 : John F. Kennedy      \t1344012\n",
      "11 : Lyndon B. Johnson    \t2288606\n",
      "12 : Richard Nixon        \t972275\n",
      "13 : Ronald Reagan        \t1010291\n",
      "14 : William J. Clinton   \t1784166\n",
      "\n",
      "Minimum number of characters per president?\n",
      "554978\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4725447, ' '), (2569974, 'e'), (2131220, 't'), (1627649, 'o'), (1609954, 'a'), (1476541, 'n'), (1405067, 'i'), (1193120, 's'), (1157275, 'r'), (1066798, 'h'), (755187, 'l'), (705084, 'd'), (548669, 'u'), (534278, 'c'), (478832, 'm'), (418343, 'w'), (393337, 'g'), (389784, 'f'), (382020, 'p'), (374441, 'y'), (281512, 'b'), (264600, ','), (263713, '.'), (240856, 'v'), (162701, 'I'), (151063, 'k'), (95930, \"'\"), (62618, 'A'), (49111, 'S'), (47587, 'T'), (44390, '-'), (40548, '7'), (39451, 'W'), (32811, 'x'), (29785, 'C'), (25503, 'j'), (23659, 'B'), (23374, 'M'), (22167, 'N'), (21726, 'q'), (21723, 'P'), (16943, 'G'), (14374, 'H'), (13620, 'U'), (12142, 'D'), (11930, 'z'), (11837, 'R'), (10677, 'F'), (10672, '\"'), (10127, 'E'), (9435, 'O'), (9370, 'Y'), (9360, ';'), (8141, '?'), (7920, 'J'), (7822, 'L'), (6390, ':'), (4737, '\\n'), (3981, '$'), (3943, 'K'), (3705, 'V'), (1853, '/'), (1066, 'Q'), (338, 'Z'), (80, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 521560 521560\n",
      "\n",
      "Total characters: 26078000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 50\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 17, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 17, 7, 6, 17, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 19, 11, 20, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 18, 1, 3, 10, 2, 1, 8, 26], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 18, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 17, 10, 1, 4, 18, 18, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 16, 2, 1, 3, 10, 2, 1, 19, 2, 4, 19, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 18, 5, 7, 3, 10, 18, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 18, 1, 4, 13, 9, 1, 18, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 18, 4, 13, 6, 12, 7, 6, 17, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few split text excerpts\n",
    "print split_text[10:15]\n",
    "print split_labels[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples; shuffle if requested\n",
    "#\n",
    "import sklearn.utils\n",
    "\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8, shuffle_p=True):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "\n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "\n",
    "    if shuffle_p:\n",
    "        test_text, test_labels = sklearn.utils.shuffle(test_text, test_labels)\n",
    "        train_text, train_labels = sklearn.utils.shuffle(train_text, train_labels)\n",
    "\n",
    "    return train_text, train_labels, test_text, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splits:\n",
      " Test =  417245 \n",
      " Train =  104315\n",
      "\n",
      "Class weights:\n",
      "{0: 0.35757318661730425, 1: 3.132822765326426, 2: 0.5636884376625394, 3: 0.9971799008185457, 4: 0.916699622110906, 5: 0.9706983994044296, 6: 2.5296774584697466, 7: 0.8687988672684304, 8: 2.2115068638363278, 9: 1.3666273623530183, 10: 1.2935422867063493, 11: 0.7596562616635261, 12: 1.7881417673780748, 13: 1.720881794935247, 14: 0.9744389173030664}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.9, shuffle_p=False)\n",
    "print \"Sample splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"\\nClass weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 17, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 17, 7, 6, 17, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 19, 11, 20, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 18, 1, 3, 10, 2, 1, 8, 26], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 18, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 17, 10, 1, 4, 18, 18, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 16, 2, 1, 3, 10, 2, 1, 19, 2, 4, 19, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 18, 5, 7, 3, 10, 18, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 18, 1, 4, 13, 9, 1, 18, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 18, 4, 13, 6, 12, 7, 6, 17, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few of the split text excerpts; \n",
    "# likely the same classes based on the split point\n",
    "print train_X[10:15]\n",
    "print train_y[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (417245, 50)\n",
      "...to  (20862250, 68)\n",
      "...and reshaping to  (417245, 50, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (104315, 50)\n",
      "...to  (5215750, 68)\n",
      "...and reshaping to  (104315, 50, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Have a again look at a few of the split and encoded text excerpts; \n",
    "# both arrays should be one-hot encoded.\n",
    "print train_X[10:11]\n",
    "print train_y[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50, 68)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                2055      \n",
      "=================================================================\n",
      "Total params: 20,687\n",
      "Trainable params: 20,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# set parameters; determined by optimization @ end\n",
    "init_modes = 'glorot_uniform'\n",
    "batch_size = unique_chars\n",
    "units = unique_chars\n",
    "dropout = 0.2\n",
    "activation = 'relu'\n",
    "merge_mode = 'sum'\n",
    "shuffle = True\n",
    "\n",
    "optimizer = Adamax(lr=0.01) \n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                              factor=0.1,\n",
    "                              patience=1, \n",
    "                              verbose=1)\n",
    "csv_logger = CSVLogger('Keras_Character_SimpleRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=units,\n",
    "                              activation=activation,\n",
    "                              recurrent_dropout=dropout,\n",
    "                              kernel_initializer=init_modes),\n",
    "                    merge_mode=merge_mode)(main_input)\n",
    "main_output = Dense(len(labels),\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer=init_modes)(rnn)\n",
    "model = Model(inputs=[main_input], outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "plot_model(model, to_file='Keras_Character_SimpleRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "417245/417245 [==============================] - 251s - loss: 2.6924 - categorical_accuracy: 0.0763   \n",
      "Epoch 2/100\n",
      "417245/417245 [==============================] - 236s - loss: 2.6583 - categorical_accuracy: 0.0918   \n",
      "Epoch 3/100\n",
      "417245/417245 [==============================] - 234s - loss: 2.6362 - categorical_accuracy: 0.0996   \n",
      "Epoch 4/100\n",
      "417245/417245 [==============================] - 246s - loss: 2.6233 - categorical_accuracy: 0.1075   \n",
      "Epoch 5/100\n",
      "329732/417245 [======================>.......] - ETA: 49s - loss: 2.6147 - categorical_accuracy: 0.1090"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=1)\n",
    "\n",
    "model.save('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model saved.\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## MODEL EVALUATION\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model re-loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(np.round(cnf_matrix,2), classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def reverse_map(map):\n",
    "    return dict((v,k) for k,v in map.iteritems())\n",
    "token_word_map = reverse_map(tokenizer.word_index)\n",
    "president_map = reverse_map(labels)\n",
    "def to_words(a):\n",
    "    return \" \".join([token_word_map[id] for id in a if id != 0])\n",
    "sample_size = 20\n",
    "def print_row(row):\n",
    "    print \"\".join(col.ljust(22) for col in row)\n",
    "print_row([\"Predicted\",\"Correct\",\"Sentence\"])\n",
    "print_row([\"----------\",\"----------\",\"---------------\"])\n",
    "sample = []\n",
    "for i in range(len(test_y_collapsed)):\n",
    "    if (pred_y_collapsed[i] != test_y_collapsed[i] and count < sample_size):\n",
    "        sample += [[president_map[pred_y_collapsed[i]], president_map[test_y_collapsed[i]], to_words(test_X[i])]]\n",
    "        count += 1\n",
    "        \n",
    "for row in sample:\n",
    "    print_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] learn_rate=0.2, dropout_rate=0.2, optimizer=Adamax, neuron_count=68, init_mode2=normal, activation=relu, init_mode1=normal, batch_size=68, epochs=2, merge_mode=concat \n",
      "Epoch 1/2\n",
      "71936/71936 [==============================] - 44s - loss: 1.2024 - acc: 0.4008 - categorical_accuracy: 0.4008    \n",
      "Epoch 2/2\n",
      "71936/71936 [==============================] - 40s - loss: 1.1940 - acc: 0.4094 - categorical_accuracy: 0.4094    \n",
      "71936/71936 [==============================] - 12s    \n",
      "[CV]  learn_rate=0.2, dropout_rate=0.2, optimizer=Adamax, neuron_count=68, init_mode2=normal, activation=relu, init_mode1=normal, batch_size=68, epochs=2, merge_mode=concat, total= 1.7min\n",
      "[CV] learn_rate=0.2, dropout_rate=0.2, optimizer=Adamax, neuron_count=68, init_mode2=normal, activation=relu, init_mode1=normal, batch_size=68, epochs=2, merge_mode=concat \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "71937/71937 [==============================] - 43s - loss: 0.0142 - acc: 0.9978 - categorical_accuracy: 0.9978    \n",
      "Epoch 2/2\n",
      "71937/71937 [==============================] - 40s - loss: 9.4863e-06 - acc: 1.0000 - categorical_accuracy: 1.0000    \n",
      "71808/71937 [============================>.] - ETA: 0s[CV]  learn_rate=0.2, dropout_rate=0.2, optimizer=Adamax, neuron_count=68, init_mode2=normal, activation=relu, init_mode1=normal, batch_size=68, epochs=2, merge_mode=concat, total= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "143873/143873 [==============================] - 86s - loss: 1.1494 - acc: 0.5402 - categorical_accuracy: 0.5402    \n",
      "Epoch 2/2\n",
      "143873/143873 [==============================] - 82s - loss: 1.1430 - acc: 0.5407 - categorical_accuracy: 0.5407    \n",
      "Best: 0.040696 using {'learn_rate': 0.2, 'dropout_rate': 0.2, 'optimizer': 'Adamax', 'neuron_count': 68, 'init_mode2': 'normal', 'activation': 'relu', 'init_mode1': 'normal', 'batch_size': 68, 'epochs': 2, 'merge_mode': 'concat'}\n",
      "0.040696 (0.040696) with: {'learn_rate': 0.2, 'dropout_rate': 0.2, 'optimizer': 'Adamax', 'neuron_count': 68, 'init_mode2': 'normal', 'activation': 'relu', 'init_mode1': 'normal', 'batch_size': 68, 'epochs': 2, 'merge_mode': 'concat'}\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## MODEL OPTIMIZATION\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# define operating vars\n",
    "#optimizer='rmsprop'\n",
    "#dropout = 0.5422412690636627\n",
    "#activation = \"relu\"\n",
    "#batch_size = 50\n",
    "#units = 50\n",
    "#shuffle = True\n",
    "#epochs = 150\n",
    "#merge_mode = 'ave'\n",
    "\n",
    "def create_model(optimizer='rmsprop', learn_rate=0.01,\n",
    "                 init_mode1='glorot_uniform', init_mode2='glorot_uniform', \n",
    "                 merge_mode='ave', activation='relu', \n",
    "                 dropout_rate=0.0, neuron_count=50):\n",
    "    # assemble & compile model\n",
    "    main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "    rnn = Bidirectional(SimpleRNN(units=neuron_count,\n",
    "                                  activation=activation,\n",
    "                                  kernel_initializer=init_mode1,\n",
    "                                  recurrent_dropout=dropout_rate),\n",
    "                        merge_mode=merge_mode)(main_input)\n",
    "    main_output = Dense(len(labels),\n",
    "                        activation='softmax',\n",
    "                        kernel_initializer=init_mode2)(rnn)\n",
    "    model = Model(inputs=[main_input], outputs=[main_output])\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy','categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "epoch = [2]\n",
    "batch_sizes = [unique_chars]\n",
    "optimizers = ['Adamax']\n",
    "init_modes1 = ['normal']\n",
    "init_modes2 = init_modes1\n",
    "merge_modes = ['concat']\n",
    "activations = ['relu']\n",
    "dropout_rates = [0.2]\n",
    "neuron_counts = [unique_chars]\n",
    "learn_rates = [0.2]\n",
    "param_grid = dict(batch_size=batch_sizes,\n",
    "                  epochs=epoch,\n",
    "                  optimizer=optimizers,\n",
    "                  learn_rate=learn_rates,\n",
    "                  init_mode1=init_modes1,\n",
    "                  init_mode2=init_modes2,\n",
    "                  merge_mode=merge_modes,\n",
    "                  activation=activations,\n",
    "                  dropout_rate=dropout_rates,\n",
    "                  neuron_count=neuron_counts)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=2, cv=2)\n",
    "grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "# summarize results\n",
    "# from http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
