{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fff79df72a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "# Include so results on different machines are (should be) the same.\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import random as rn\n",
    "rn.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Keras_BagnallCharacter_SimpleRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 speeches for 14 presidents.\n",
      "Loaded 1764 speeches for 15 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\n",
    "    \"Barack Obama\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"Dwight D. Eisenhower\",\n",
    "    \"Franklin D. Roosevelt\",\n",
    "    \"George Bush\",\n",
    "    \"George W. Bush\",\n",
    "    \"Gerald R. Ford\",\n",
    "    \"Harry S. Truman\",\n",
    "    \"Herbert Hoover\",\n",
    "    \"Jimmy Carter\",\n",
    "    \"John F. Kennedy\",\n",
    "    \"Lyndon B. Johnson\",\n",
    "    \"Richard Nixon\",\n",
    "    \"Ronald Reagan\",\n",
    "    \"William J. Clinton\"\n",
    "]\n",
    "\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\n",
    "    \"Obama\": \"Barack Obama\",\n",
    "    \"Trump\": \"Donald J. Trump\",\n",
    "    \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "    \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "    \"Bush\": \"George Bush\",\n",
    "    \"WBush\": \"George W. Bush\",\n",
    "    \"Ford\": \"Gerald R. Ford\",\n",
    "    \"Truman\": \"Harry S. Truman\",\n",
    "    \"Hoover\": \"Herbert Hoover\",\n",
    "    \"Carter\": \"Jimmy Carter\",\n",
    "    \"Kennedy\": \"John F. Kennedy\",\n",
    "    \"Johnson\": \"Lyndon B. Johnson\",\n",
    "    \"Nixon\": \"Richard Nixon\",\n",
    "    \"Reagan\": \"Ronald Reagan\",\n",
    "    \"Clinton\": \"William J. Clinton\"\n",
    "}\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# from collections import Counter\n",
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "# tagger = PerceptronTagger()\n",
    "\n",
    "# assess_text = word_tokenize(\" \".join(loaded_text))\n",
    "\n",
    "# tagged_sent = tagger.tag(assess_text) \n",
    "# propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "\n",
    "# print Counter(propernouns).most_common()[:-1000-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_}+\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    # \"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '\\'', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('--', '-', loaded_text[x])\n",
    "    loaded_text[x] = re.sub('\\n\\n', '\\n', loaded_text[x])\n",
    "    # \"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]+', '7', loaded_text[x])\n",
    "    # \"...any character with a frequency lower than 1 in 10,000 is discarded.\"\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    # \"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    " \n",
    "    # REPLACE WORD IN ALL CAPS with <space>; headers\n",
    "    loaded_text[x] = re.sub('[A-Z]{2,}','', loaded_text[x])\n",
    "\n",
    "print \"Replacements complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Before answering questions this evening, I would like to say a few words about the rescue mission in Iran. I share the disappointment of the American people that this rescue mission was not successful, and I also share the grief of our Nation because we had Americans who were casualties in this effort to seek freedom for their fellow citizens who have been held hostage for so long. But I also share a deep pride in the commitment and courage and the integrity and the competence and the determination of those who went on this mission. They were prepared to do their duty, and they did their duty. I can think of no higher compliment for a Commander in Chief to pay to brave men. It was my responsibility as President to launch this mission. It w\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a scrubbed text excerpt\n",
    "print loaded_text[200][:750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4869126\n",
      "1  : Donald J. Trump      \t555750\n",
      "2  : Dwight D. Eisenhower \t3085442\n",
      "3  : Franklin D. Roosevelt \t1746653\n",
      "4  : George Bush          \t1899100\n",
      "5  : George W. Bush       \t1792991\n",
      "6  : Gerald R. Ford       \t706711\n",
      "7  : Harry S. Truman      \t2002254\n",
      "8  : Herbert Hoover       \t800912\n",
      "9  : Jimmy Carter         \t1290474\n",
      "10 : John F. Kennedy      \t1345102\n",
      "11 : Lyndon B. Johnson    \t2326890\n",
      "12 : Richard Nixon        \t990798\n",
      "13 : Ronald Reagan        \t1010956\n",
      "14 : William J. Clinton   \t1786719\n",
      "\n",
      "Minimum number of characters per president?\n",
      "555750\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Join all speeches into one massive per president\n",
    "#  for later processing\n",
    "#\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 68\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4725447, ' '), (2569974, 'e'), (2131220, 't'), (1627649, 'o'), (1609954, 'a'), (1476541, 'n'), (1405067, 'i'), (1193120, 's'), (1157275, 'r'), (1066798, 'h'), (755187, 'l'), (705084, 'd'), (548669, 'u'), (534278, 'c'), (478832, 'm'), (418343, 'w'), (393337, 'g'), (389784, 'f'), (382020, 'p'), (374441, 'y'), (281512, 'b'), (264600, ','), (263713, '.'), (240856, 'v'), (175656, 'I'), (151063, 'k'), (95930, \"'\"), (74786, 'A'), (59022, 'T'), (58392, 'S'), (44390, '-'), (40592, 'W'), (40548, '7'), (35537, 'C'), (33805, 'N'), (32811, 'x'), (27664, 'M'), (25791, 'P'), (25503, 'j'), (25399, 'B'), (23916, 'E'), (21726, 'q'), (20252, 'R'), (19956, 'O'), (19583, 'G'), (17171, 'H'), (16245, 'U'), (15855, 'D'), (13886, 'F'), (12873, 'L'), (11930, 'z'), (10681, 'Y'), (10672, '\"'), (9360, ';'), (8141, '?'), (8075, 'J'), (6390, ':'), (5293, 'V'), (4737, '\\n'), (4486, 'K'), (3981, '$'), (1853, '/'), (1349, 'Q'), (446, 'Z'), (421, 'X'), (6, '#'), (4, '\\\\')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tokenize words into chars\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "# there's an oddity in the encoding for some reason where a len+1 character occurs\n",
    "unique_chars = len(tokenizer.word_counts)+1\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence total count; subsequence label total count: 524192 524192\n",
      "\n",
      "Total characters: 26209600\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 50\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx]#[:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Subsequence total count; subsequence label total count:\", len( split_text ), len( split_labels )\n",
    "print \"\\nTotal characters:\", len( split_text ) * max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 17, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 17, 7, 6, 17, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 19, 11, 20, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 18, 1, 3, 10, 2, 1, 8, 26], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 18, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 17, 10, 1, 4, 18, 18, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 16, 2, 1, 3, 10, 2, 1, 19, 2, 4, 19, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 18, 5, 7, 3, 10, 18, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 18, 1, 4, 13, 9, 1, 18, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 18, 4, 13, 6, 12, 7, 6, 17, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few split text excerpts\n",
    "print split_text[10:15]\n",
    "print split_labels[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split amongst speaker samples, not the whole population of samples; shuffle if requested\n",
    "#\n",
    "import sklearn.utils\n",
    "\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8, shuffle_p=True):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "\n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "\n",
    "    if shuffle_p:\n",
    "        test_text, test_labels = sklearn.utils.shuffle(test_text, test_labels)\n",
    "        train_text, train_labels = sklearn.utils.shuffle(train_text, train_labels)\n",
    "\n",
    "    return train_text, train_labels, test_text, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splits:\n",
      " Test =  419349 \n",
      " Train =  104843\n",
      "\n",
      "Class weights:\n",
      "{0: 0.3588550157242796, 1: 3.144017094017094, 2: 0.5663128468986752, 3: 1.000379302941387, 4: 0.9200789863419451, 5: 0.9745389897863144, 6: 2.472503758733528, 7: 0.8726620052441004, 8: 2.181723115342594, 9: 1.3540272194507676, 10: 1.2990381487849079, 11: 0.7509360982030138, 12: 1.763600807469089, 13: 1.7283833075734159, 14: 0.9779480183300101}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# split data smartly\n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, \n",
    "                                                    labels, train_pct=0.8, shuffle_p=False)\n",
    "print \"Sample splits:\\n Test = \", len(train_X), \"\\n Train = \", len(test_X)##\n",
    "\n",
    "# compute class weights to account for imbalanced classes\n",
    "y_weights = (class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)).tolist()\n",
    "y_weights = dict(zip(sorted(labels.values()), y_weights))\n",
    "print \"\\nClass weights:\\n\", y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 7, 6, 17, 1, 14, 11, 4, 13, 12, 8, 1, 5, 6, 12, 1, 9, 5, 17, 7, 6, 17, 1, 8, 3, 4, 9, 15, 8, 23, 1, 28, 3, 1, 3, 10, 2, 8, 2, 1, 15, 4, 15, 2, 6, 3, 8, 22, 1, 28], [15, 2, 9, 7, 14, 5, 1, 10, 5, 8, 1, 14, 5, 9, 9, 7, 2, 12, 1, 4, 6, 1, 6, 4, 3, 1, 8, 7, 15, 19, 11, 20, 1, 21, 2, 14, 5, 13, 8, 2, 1, 4, 18, 1, 3, 10, 2, 1, 8, 26], [7, 11, 11, 1, 4, 9, 1, 24, 7, 8, 7, 4, 6, 1, 4, 18, 1, 3, 10, 4, 8, 2, 1, 7, 6, 1, 10, 7, 17, 10, 1, 4, 18, 18, 7, 14, 2, 22, 1, 21, 13, 3, 1, 21, 2, 14, 5, 13, 8, 2], [1, 16, 2, 1, 3, 10, 2, 1, 19, 2, 4, 19, 11, 2, 1, 10, 5, 24, 2, 1, 9, 2, 15, 5, 7, 6, 2, 12, 1, 18, 5, 7, 3, 10, 18, 13, 11, 1, 3, 4, 1, 3, 10, 2, 1, 7, 12, 2, 5, 11], [8, 1, 4, 18, 1, 4, 13, 9, 1, 18, 4, 9, 2, 21, 2, 5, 9, 8, 1, 5, 6, 12, 1, 3, 9, 13, 2, 1, 3, 4, 1, 4, 13, 9, 1, 18, 4, 13, 6, 12, 7, 6, 17, 1, 12, 4, 14, 13, 15, 2]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at a few of the split text excerpts; \n",
    "# likely the same classes based on the split point\n",
    "print train_X[10:15]\n",
    "print train_y[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train_X with dimensions  (419349, 50)\n",
      "...to  (20967450, 68)\n",
      "...and reshaping to  (419349, 50, 68)\n",
      "\n",
      "Encoding test_X with dimensions  (104843, 50)\n",
      "...to  (5242150, 68)\n",
      "...and reshaping to  (104843, 50, 68)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# One-hot encoding classes & samples\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "# one-hot encode samples\n",
    "train_X = np.array(train_X)\n",
    "orig_train_X_size=train_X.shape[0]\n",
    "print \"Encoding train_X with dimensions \", train_X.shape\n",
    "train_X = to_categorical(train_X, num_classes=unique_chars)\n",
    "print \"...to \", train_X.shape\n",
    "train_X = np.reshape(train_X,(orig_train_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", train_X.shape\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "orig_test_X_size=test_X.shape[0]\n",
    "print \"\\nEncoding test_X with dimensions \", test_X.shape\n",
    "test_X = to_categorical(test_X, num_classes=unique_chars)\n",
    "print \"...to \", test_X.shape\n",
    "test_X = np.reshape(test_X,(orig_test_X_size,max_seq_len,unique_chars))\n",
    "print \"...and reshaping to \", test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Have a again look at a few of the split and encoded text excerpts; \n",
    "# both arrays should be one-hot encoded.\n",
    "print train_X[10:11]\n",
    "print train_y[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom activation from Bagnall 2015\n",
    "#  we were never able to get this to work; either nan'ed or never converged\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def ReSQRT(x):\n",
    "    cond = tf.less_equal(x, 0.0)\n",
    "    return tf.where(cond, x * 0.0, tf.sqrt(x+1)-1)\n",
    "\n",
    "get_custom_objects().update({'ReSQRT': ReSQRT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bagnall proposes that the following possible values contribute to the success of the model:\n",
    "\n",
    "| meta-parameter                  \t| typical value                      \t|\n",
    "|---------------------------------\t|------------------------------------\t|\n",
    "| initial adagrad learning scale  \t| 0.1, 0.14, 0.2, 0.3                \t|\n",
    "| initial leakage between classes \t| 1/4N to 5/N                        \t|\n",
    "| leakage decay (per sub-epoch)   \t| 0.67 to 0.9                        \t|\n",
    "| hidden neurons                  \t| 79, 99, 119, 139                   \t|\n",
    "| presynaptic noise Ïƒ             \t| 0, 0.1, 0.2, 0.3, 0.5              \t|\n",
    "| sub-epochs                      \t| 6 to 36                            \t|\n",
    "| text direction                  \t| forward or backward                \t|\n",
    "| text handling                   \t| sequential, concatenated, balanced \t|\n",
    "| initialisation                  \t| gaussian, zero                     \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 68)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50)                11900     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                765       \n",
      "=================================================================\n",
      "Total params: 12,665\n",
      "Trainable params: 12,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "254750/419349 [=================>............] - ETA: 143s - loss: 2.6807 - categorical_accuracy: 0.0823"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e705d5aaae4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           verbose=1)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keras_BagnallCharacter_SimpleRNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/keras_tf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##\n",
    "## BASELINE\n",
    "##\n",
    "from keras.layers import Input, Dense, SimpleRNN, Bidirectional, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define operating vars\n",
    "optimizer='rmsprop'\n",
    "dropout = 0.5422412690636627\n",
    "activation = \"relu\"\n",
    "batch_size = 50\n",
    "units = 50\n",
    "shuffle = True\n",
    "epochs = 50\n",
    "merge_mode = 'ave'\n",
    "\n",
    "# define any callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='categorical_accuracy', \n",
    "                              factor=0.5,\n",
    "                              patience=1, \n",
    "                              verbose=1)\n",
    "csv_logger = CSVLogger('Keras_Character_SimpleRNN.log')\n",
    "\n",
    "# assemble & compile model\n",
    "main_input = Input(shape=(max_seq_len,unique_chars,))\n",
    "rnn = Bidirectional(SimpleRNN(units=units,\n",
    "                              activation=activation),\n",
    "                    merge_mode=merge_mode)(main_input)\n",
    "drop = Dropout(dropout)(rnn)\n",
    "main_output = Dense(len(labels),\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='random_normal')(drop)\n",
    "model = Model(inputs=[main_input], outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy'])\n",
    "plot_model(model, to_file='Keras_Character_SimpleRNN.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# train\n",
    "model.fit([np.array(train_X)],\n",
    "          [np.array(train_y)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          class_weight = y_weights,\n",
    "          callbacks=[reduce_lr, csv_logger],\n",
    "          verbose=1)\n",
    "\n",
    "model.save('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model saved.\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load computed model\n",
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the one trained\n",
    "model = load_model('Keras_Character_SimpleRNN.h5')\n",
    "print (\"Model re-loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "print \"\\n\\nDone prediction.\"\n",
    "\n",
    "print \"\\nAUC = \", metrics.roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "#   from scikit-learn examples @\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        print(np.sum(cm,axis=0))\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(np.round(cnf_matrix,2), classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def reverse_map(map):\n",
    "    return dict((v,k) for k,v in map.iteritems())\n",
    "token_word_map = reverse_map(tokenizer.word_index)\n",
    "president_map = reverse_map(labels)\n",
    "def to_words(a):\n",
    "    return \" \".join([token_word_map[id] for id in a if id != 0])\n",
    "sample_size = 20\n",
    "def print_row(row):\n",
    "    print \"\".join(col.ljust(22) for col in row)\n",
    "print_row([\"Predicted\",\"Correct\",\"Sentence\"])\n",
    "print_row([\"----------\",\"----------\",\"---------------\"])\n",
    "sample = []\n",
    "for i in range(len(test_y_collapsed)):\n",
    "    if (pred_y_collapsed[i] != test_y_collapsed[i] and count < sample_size):\n",
    "        sample += [[president_map[pred_y_collapsed[i]], president_map[test_y_collapsed[i]], to_words(test_X[i])]]\n",
    "        count += 1\n",
    "        \n",
    "for row in sample:\n",
    "    print_row(row)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
