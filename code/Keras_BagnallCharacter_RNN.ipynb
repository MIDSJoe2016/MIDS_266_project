{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 speeches for 2 presidents.\n",
      "Loaded 1435 speeches for 13 presidents.\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json, re, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_verbose = 0\n",
    "loaded_labels = []\n",
    "loaded_text = []\n",
    "presidents = [\"Barack Obama\",\n",
    "          \"Donald J. Trump\",\n",
    "          \"Dwight D. Eisenhower\",\n",
    "          \"Franklin D. Roosevelt\",\n",
    "          \"George Bush\",\n",
    "          \"George W. Bush\",\n",
    "#          \"Gerald R. Ford\",\n",
    "          \"Harry S. Truman\",\n",
    "#          \"Herbert Hoover\",\n",
    "          \"Jimmy Carter\",\n",
    "          \"John F. Kennedy\",\n",
    "          \"Lyndon B. Johnson\",\n",
    "          \"Richard Nixon\",\n",
    "          \"Ronald Reagan\",\n",
    "          \"William J. Clinton\"]\n",
    "labels = {}\n",
    "for idx, name in enumerate(presidents):\n",
    "    labels[name] = idx\n",
    "\n",
    "# load raw text files straight in, no parsing\n",
    "file_to_label = {\"Obama\": \"Barack Obama\", \n",
    "                     \"Trump\": \"Donald J. Trump\",\n",
    "                     \"Eisenhower\": \"Dwight D. Eisenhower\",\n",
    "                     \"Roosevelt\": \"Franklin D. Roosevelt\",\n",
    "                     \"Bush\": \"George Bush\",\n",
    "                     \"WBush\": \"George W. Bush\",\n",
    "#                     \"Ford\": \"Gerald R. Ford\",\n",
    "                     \"Truman\": \"Harry S. Truman\",\n",
    "#                     \"Hoover\": \"Herbert Hoover\",\n",
    "                     \"Carter\": \"Jimmy Carter\",\n",
    "                     \"Kennedy\": \"John F. Kennedy\",\n",
    "                     \"Johnson\": \"Lyndon B. Johnson\",\n",
    "                     \"Nixon\": \"Richard Nixon\",\n",
    "                     \"Reagan\": \"Ronald Reagan\",\n",
    "                     \"Clinton\": \"William J. Clinton\"\n",
    "                    }\n",
    "\n",
    "directory = \"../data/processed/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\"\n",
    "# processed2 now contains files generated from unprocessed\n",
    "directory = \"../data/processed3/\"\n",
    "for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
    "    arr = filename.replace(directory,'').split(\"_\")\n",
    "    if any(prefix in arr[0] for prefix in file_to_label.keys()):\n",
    "        loaded_labels = loaded_labels + [labels[file_to_label[arr[0]]]]\n",
    "        raw = open(filename).read().decode(\"UTF-8\").encode(\"ascii\",\"ignore\")\n",
    "        loaded_text = loaded_text + [raw] \n",
    "\n",
    "print \"Loaded\", len(loaded_text), \"speeches for\", len(set(loaded_labels)), \"presidents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Bagnall 2015 text pre-processing\n",
    "#\n",
    "from string import maketrans\n",
    "import re\n",
    "\n",
    "chars_to_replace = \"[]%!()>=*&_\\n\"\n",
    "sub_chars = len(chars_to_replace) * \" \"\n",
    "trantab = maketrans(chars_to_replace, sub_chars)\n",
    "for x in range(0,len(loaded_text)):\n",
    "    #\"Various rare characters that seemed largely equivalent are mapped together...\"\n",
    "    loaded_text[x] = re.sub('`', '', loaded_text[x])\n",
    "    #\"...all digits in all languages are mapped to 7\"\n",
    "    loaded_text[x] = re.sub('[0-9]', '7', loaded_text[x])\n",
    "    #\"...any character with a frequency lower than 1 in 10,000 is discarded.\" (+ \\n)\n",
    "    loaded_text[x] = loaded_text[x].translate(trantab)\n",
    "    #\"Runs of whitespace are collapsed into a single space.\"\n",
    "    loaded_text[x] = re.sub(' +', ' ', loaded_text[x])\n",
    "\n",
    "print \"Replacements complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many characters of text per president?\n",
      "0  : Barack Obama         \t4891166\n",
      "1  : Donald J. Trump      \t445782\n",
      "2  : Dwight D. Eisenhower \t3109439\n",
      "3  : Franklin D. Roosevelt \t2125124\n",
      "4  : George Bush          \t1922344\n",
      "5  : George W. Bush       \t1763481\n",
      "6  : Harry S. Truman      \t2066478\n",
      "7  : Jimmy Carter         \t1294669\n",
      "8  : John F. Kennedy      \t1352086\n",
      "9  : Lyndon B. Johnson    \t2356175\n",
      "10 : Richard Nixon        \t992093\n",
      "11 : Ronald Reagan        \t987802\n",
      "12 : William J. Clinton   \t1762034\n",
      "\n",
      "Minimum number of characters per president?\n",
      "445782\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "# compress all speeches down into one massive per president\n",
    "compressed_text = [None]*(len(labels))\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    compressed_text[value] = \"\"\n",
    "    for idx in range(0,len(loaded_text)):\n",
    "        if (loaded_labels[idx] == value):\n",
    "            compressed_text[value] = compressed_text[value] + loaded_text[idx] + \" \"\n",
    "            \n",
    "print \"How many characters of text per president?\"\n",
    "for key, value in sorted(labels.iteritems()):\n",
    "    print str(value).ljust(2), \":\", key.ljust(20), \"\\t\", len(compressed_text[value])\n",
    "\n",
    "label_min_chars = len(min(compressed_text, key=len))\n",
    "print \"\\nMinimum number of characters per president?\"\n",
    "print label_min_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique char count: 66\n",
      "\n",
      "Chars w/ counts:\n",
      "[(4497030, ' '), (2450414, 'e'), (2033706, 't'), (1549204, 'o'), (1531378, 'a'), (1408772, 'n'), (1336422, 'i'), (1138825, 's'), (1100960, 'r'), (1017877, 'h'), (717297, 'l'), (674129, 'd'), (524147, 'u'), (506926, 'c'), (452461, 'm'), (402150, 'w'), (375828, 'g'), (369501, 'f'), (361094, 'p'), (358432, 'y'), (266698, 'b'), (259049, '.'), (254447, ','), (226984, 'v'), (169322, 'I'), (145733, 'k'), (95687, \"'\"), (85163, '7'), (69999, 'A'), (62624, 'T'), (58505, 'S'), (42713, '-'), (39923, 'W'), (34953, 'N'), (32800, 'C'), (32236, 'E'), (31326, 'x'), (30287, 'P'), (27134, 'M'), (24669, 'B'), (24321, 'j'), (21883, 'R'), (20959, 'H'), (20485, 'q'), (18639, 'D'), (17793, 'G'), (17238, 'O'), (14935, 'U'), (12348, 'F'), (11775, ':'), (11417, 'L'), (11171, 'z'), (10683, '\"'), (10496, '?'), (10052, 'Y'), (9480, ';'), (8199, 'J'), (4691, 'V'), (4551, 'Q'), (4350, 'K'), (3474, '$'), (1824, '/'), (424, 'Z'), (361, '\\\\'), (318, 'X'), (1, '}')]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Tokenize into characters\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(compressed_text)\n",
    "tokenized_text = tokenizer.texts_to_sequences(compressed_text)\n",
    "\n",
    "unique_chars = len(tokenizer.word_counts)\n",
    "\n",
    "print \"Unique char count:\", unique_chars\n",
    "print \"\\nChars w/ counts:\"\n",
    "print sorted(((v,k) for k,v in tokenizer.word_counts.iteritems()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample, label sizes: 57941 57941\n",
      "\n",
      "Total split groups: 579 = ( 57941 / 100 )\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split speeches into subsequences \n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "def splits(_list, _split_size):\n",
    "    output_list = []\n",
    "    for idx in range(0, len(_list), _split_size):\n",
    "        if (idx + _split_size) <= len(_list):\n",
    "            output_list.append(_list[idx:idx + _split_size])\n",
    "    return output_list\n",
    "\n",
    "max_seq_len = 100\n",
    "\n",
    "# create new speech/label holders\n",
    "split_text = []\n",
    "split_labels = []\n",
    "\n",
    "for idx in range(0, len(tokenized_text)):\n",
    "    current_label = idx\n",
    "    current_speech = tokenized_text[idx][:label_min_chars]\n",
    "    current_splits = splits(current_speech, max_seq_len)\n",
    "    split_text.extend(current_splits)\n",
    "    split_labels.extend([current_label] * len(current_splits))\n",
    "\n",
    "print \"Sample, label sizes:\", len( split_text ), len( split_labels )\n",
    "split_size = len( split_text ) / max_seq_len\n",
    "print \"\\nTotal split groups:\", split_size, \"= (\",len( split_text ),\"/\",max_seq_len,\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split amongst speaker samples, not the whole population of samples\n",
    "def split_test_train(input_text, input_labels, labels, train_pct=0.8):\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key, value in sorted(labels.iteritems()):\n",
    "        # grab all values of a specific label\n",
    "        subset_text = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_text))\n",
    "        subset_labels = list(itemgetter(*[idx for idx, label in enumerate(input_labels) if label == value ])(input_labels))\n",
    "        \n",
    "        cut_pos = int(train_pct * len(subset_text))\n",
    "        train_text = train_text + subset_text[:cut_pos]\n",
    "        train_labels = train_labels + subset_labels[:cut_pos]\n",
    "        test_text = test_text + subset_text[cut_pos:]\n",
    "        test_labels = test_labels + subset_labels[cut_pos:]\n",
    "        \n",
    "    return train_text,train_labels,test_text,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: test =  46345 train =  11596\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prep test/train\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#  \n",
    "train_X, train_y, test_X, test_y = split_test_train(split_text, split_labels, labels)\n",
    "\n",
    "print \"Splits: test = \", len(train_X), \"train = \", len(test_X)\n",
    "\n",
    "# one-hot encode classes\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom activation from Bagnall 2015\n",
    "import tensorflow as tf\n",
    "\n",
    "def ReSQRT(x):\n",
    "    cond = tf.less_equal(x, 0)\n",
    "    result = tf.where(cond, x + 0.0, tf.sqrt(x+1)-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          6600      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                1313      \n",
      "=================================================================\n",
      "Total params: 28,013\n",
      "Trainable params: 28,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.5647 - categorical_accuracy: 0.0826    \n",
      "Epoch 2/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.5981 - categorical_accuracy: 0.0936    \n",
      "Epoch 3/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.5530 - categorical_accuracy: 0.1021    \n",
      "Epoch 4/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.5394 - categorical_accuracy: 0.1053    \n",
      "Epoch 5/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.5175 - categorical_accuracy: 0.1153    \n",
      "Epoch 6/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.5096 - categorical_accuracy: 0.1207    \n",
      "Epoch 7/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.5063 - categorical_accuracy: 0.1245    \n",
      "Epoch 8/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.5226 - categorical_accuracy: 0.1173    \n",
      "Epoch 9/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.5013 - categorical_accuracy: 0.1348    \n",
      "Epoch 10/100\n",
      "46345/46345 [==============================] - 41s - loss: 2.4546 - categorical_accuracy: 0.1436    \n",
      "Epoch 11/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.4679 - categorical_accuracy: 0.1476    \n",
      "Epoch 12/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.4418 - categorical_accuracy: 0.1565    \n",
      "Epoch 13/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.4263 - categorical_accuracy: 0.1594    \n",
      "Epoch 14/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.4134 - categorical_accuracy: 0.1609    \n",
      "Epoch 15/100\n",
      "46345/46345 [==============================] - 47s - loss: 2.3811 - categorical_accuracy: 0.1699    \n",
      "Epoch 16/100\n",
      "46345/46345 [==============================] - 44s - loss: 2.3562 - categorical_accuracy: 0.1750    \n",
      "Epoch 17/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.3409 - categorical_accuracy: 0.1791    \n",
      "Epoch 18/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.3574 - categorical_accuracy: 0.1778    \n",
      "Epoch 19/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.3249 - categorical_accuracy: 0.1844    \n",
      "Epoch 20/100\n",
      "46345/46345 [==============================] - 43s - loss: 2.3278 - categorical_accuracy: 0.1843    \n",
      "Epoch 21/100\n",
      "46345/46345 [==============================] - 42s - loss: 2.3110 - categorical_accuracy: 0.1883    \n",
      "Epoch 22/100\n",
      "24250/46345 [==============>...............] - ETA: 20s - loss: 2.2812 - categorical_accuracy: 0.1965"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, SimpleRNN, Dropout\n",
    "from keras.optimizers import Adagrad, adam\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(unique_chars,100,input_length=max_seq_len))\n",
    "model.add(SimpleRNN(100,activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "optimizer = Adagrad(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print \"Evaluating test data...\"\n",
    "loss_and_metrics = model.evaluate(test_X, test_y)\n",
    "print model.metrics_names\n",
    "print loss_and_metrics\n",
    "\n",
    "# Make some predictions\n",
    "print \"\\nPredicting using test data...\"\n",
    "pred_y = model.predict(test_X, batch_size=batch_size, verbose=1)\n",
    "pred_y_collapsed = np.argmax(pred_y, axis=1)\n",
    "test_y_collapsed = np.argmax(test_y, axis=1)\n",
    "\n",
    "print \"Done prediction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from scikit-learn examples @\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_y_collapsed, pred_y_collapsed)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# #Plot normalized confusion matrix\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=(sorted(labels, key=labels.get)), normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
